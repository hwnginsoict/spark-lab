{"cells":[{"cell_type":"markdown","metadata":{"id":"VJaLzF4eE7Va"},"source":["<h2 style=\"text-align:center;font-size:200%;;\">Advanced Pyspark for Exploratory Data Analysis </h2>\n","<h3  style=\"text-align:center;\"><span class=\"label label-success\">Lazy evaluation</span> <span class=\"label label-success\">UDF registering</span> <span class=\"label label-success\">Spark RDD</span> <span class=\"label label-success\">Data Processing</span> <span class=\"label label-success\">Explored Analysis</span> <span class=\"label label-success\">Visualization</span></h3>"]},{"cell_type":"markdown","metadata":{"id":"P4yxwtyNE7Vb"},"source":["## The benefit of learning Pyspark\n","\n","* If your team are planning to do project with really large dataset, Spark is really great choice because of its power for <font color=\"red\"><b>handling bigdata</b></font> .\n","Here is the comparision of Pyspark vs Pandas from Databrick:  \n","![Spark_vs_Pandas.JPG](attachment:Spark_vs_Pandas.JPG)\n","Source: https://databricks.com/blog/2018/05/03/benchmarking-apache-spark-on-a-single-node-machine.html\n","\n","\n","* If you are planning to <font color=\"red\"><b>land a job</b></font> in a company with really big data ecosystem,  be familiar with Spark will be a <font color=\"red\"><b>good plus</b></font>  and makes you different than other applicants. (that's from my experience)\n","\n","\n","In case you are new to pyspark, you can check it first,\n","an introduction guide to Pyspark :https://www.kaggle.com/tientd95/pyspark-for-data-science"]},{"cell_type":"markdown","metadata":{"id":"4_TuRhZbE7Vb"},"source":["<a class=\"anchor\" id=\"0.1\"></a>\n","[](http://)\n","\n","# **Table of Contents**\n","\n","\n","1.\t[Initialize pyspark framework and load data into pyspark's dataframe](#1)\n","2.\t[Overview of Dataset](#2)\n","3.\t[Detect missing values and abnormal zeroes](#3)\n","4.\t[Pyspark lazy evaluation](#4)\n","5.\t[Explolatory Data analysis in Pyspark](#5)    \n","6.\t[Unstack pyspark dataframe](#6)\n","7.\t[Pyspark UDF Registering](#7)\n","8.\t[Convert row objects to Spark Resilient Distributed Dataset (RDD)](#8)\n"]},{"cell_type":"markdown","metadata":{"id":"vdP2vLAZE7Vb"},"source":["# **1. Initialize pyspark framework and load data into pyspark's dataframe** <a class=\"anchor\" id=\"1\"></a>\n","\n","[Go back to table of contents](#0.1)\n","\n","\n","The dataset we use is from : Datasource: https://sites.google.com/eng.ucsd.edu/fitrec-project/home \n","\n","This dataset is about calculating the heart rate of people, along with other relating features: gender, weather condition, sport type, GPS, etc"]},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-output":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"IhO0-PY8E7Vc","executionInfo":{"status":"ok","timestamp":1675390946329,"user_tz":-420,"elapsed":61860,"user":{"displayName":"Tran Viet Trung","userId":"16281260022837771696"}},"outputId":"227beb7d-1ea7-4ff6-fd11-62354ebc99e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=d62e1b1f355bdcd3a77d7d33348afc18244e8000b00c5a860bc92c1358e22c3d\n","  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"]}],"source":["!pip install pyspark;"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_jpa3S7JE7Vc","executionInfo":{"status":"ok","timestamp":1675391025858,"user_tz":-420,"elapsed":826,"user":{"displayName":"Tran Viet Trung","userId":"16281260022837771696"}}},"outputs":[],"source":["# Import other modules not related to PySpark\n","import os\n","import sys\n","import pandas as pd\n","from pandas import DataFrame\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as mtick\n","import matplotlib\n","from mpl_toolkits.mplot3d import Axes3D\n","import math\n","from IPython.core.interactiveshell import InteractiveShell\n","from datetime import *\n","import statistics as stats\n","# This helps auto print out the items without explixitly using 'print'\n","InteractiveShell.ast_node_interactivity = \"all\" \n","%matplotlib inline"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"id":"_mFokdkcE7Vd","executionInfo":{"status":"error","timestamp":1675391045573,"user_tz":-420,"elapsed":12342,"user":{"displayName":"Tran Viet Trung","userId":"16281260022837771696"}},"outputId":"2bcde7a1-52d0-483d-d86e-88b2ccd3f117"},"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-c49bea4568a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mfilename_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../input/fitrec-dataset/endomondoHR.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Load the main data set into pyspark data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DROPMALFORMED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data frame type: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/input/fitrec-dataset/endomondoHR.json"]}],"source":["# Import PySpark related modules\n","import pyspark\n","from pyspark.rdd import RDD\n","from pyspark.sql import Row\n","from pyspark.sql import DataFrame\n","from pyspark.sql import SparkSession\n","from pyspark.sql import SQLContext\n","from pyspark.sql import functions\n","from pyspark.sql.functions import lit, desc, col, size, array_contains\\\n",", isnan, udf, hour, array_min, array_max, countDistinct\n","from pyspark.sql.types import *\n","\n","MAX_MEMORY = '15G'\n","# Initialize a spark session.\n","conf = pyspark.SparkConf().setMaster(\"local[*]\") \\\n","        .set('spark.executor.heartbeatInterval', 10000) \\\n","        .set('spark.network.timeout', 10000) \\\n","        .set(\"spark.core.connection.ack.wait.timeout\", \"3600\") \\\n","        .set(\"spark.executor.memory\", MAX_MEMORY) \\\n","        .set(\"spark.driver.memory\", MAX_MEMORY)\n","def init_spark():\n","    spark = SparkSession \\\n","        .builder \\\n","        .appName(\"Pyspark guide\") \\\n","        .config(conf=conf) \\\n","        .getOrCreate()\n","    return spark\n","\n","spark = init_spark()\n","filename_data = '../input/fitrec-dataset/endomondoHR.json'\n","# Load the main data set into pyspark data frame \n","df = spark.read.json(filename_data, mode=\"DROPMALFORMED\")\n","print('Data frame type: ' + str(type(df)))"]},{"cell_type":"markdown","metadata":{"id":"eEKdMv16E7Vd"},"source":["# **2. Overview of Dataset** <a class=\"anchor\" id=\"2\"></a>\n","\n","[Go back to table of contents](#0.1)\n","\n","### Schema, columns & datatypes of the data set:\n","   *The data set has both single value columns (int, string) and columns made of arrays/list.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxjHn-j1E7Vd"},"outputs":[],"source":["print('Data overview')\n","df.printSchema()\n","print('Columns overview')\n","pd.DataFrame(df.dtypes, columns = ['Column Name','Data type'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khO2IK0qE7Vd"},"outputs":[],"source":["print('Data frame describe (string and numeric columns only):')\n","df.describe().toPandas()\n","\n","print(f'There are total {df.count()} row, Let print first 2 data rows:')\n","df.limit(2).toPandas()"]},{"cell_type":"markdown","metadata":{"id":"INf3bdQBE7Vd"},"source":["# **3. Detect missing values and abnormal zeroes** <a class=\"anchor\" id=\"3\"></a>\n","\n","[Go back to table of contents](#0.1)\n","\n","After having a first sight of the columns, the first thing we should check is if the data set having any missing value.\n","- For string columns, we check for `None` and `null`\n","- For numeric columns, we check for zeroes and `NaN`\n","- For array type columns, we check if the array contain zeroes or `NaN`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJ6uWvwpE7Vd"},"outputs":[],"source":["print('Columns overview')\n","pd.DataFrame(df.dtypes, columns = ['Column Name','Data type'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UpuKR3JE7Vd"},"outputs":[],"source":["string_columns = ['gender', 'sport', 'url']\n","numeric_columns = ['id','userId']\n","array_columns = ['altitude', 'heart_rate', 'latitude', 'longitude', 'speed', 'timestamp']\n","missing_values = {} \n","for index, column in enumerate(df.columns):\n","    if column in string_columns:    # check string columns with None and Null values\n","#         missing_count = df.filter(col(column).eqNullSafe(None) | col(column).isNull()).count()\n","#         missing_values.update({column: missing_count})\n","        missing_count = df.filter(col(column).eqNullSafe(None) | col(column).isNull()).count()\n","        missing_values.update({column:missing_count})\n","    if column in numeric_columns:  # check zeroes, None, NaN\n","        missing_count = df.where(col(column).isin([0,None,np.nan])).count()\n","        missing_values.update({column:missing_count})\n","    if column in array_columns:  # check zeros and NaN\n","        missing_count = df.filter(array_contains(df[column], 0) | array_contains(df[column], np.nan)).count()\n","        missing_values.update({column:missing_count})\n","missing_df = pd.DataFrame.from_dict([missing_values])\n","missing_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vT3FpGt-E7Ve"},"outputs":[],"source":["# We create new column to count the number of timestamps recorded per row/workout, named as 'PerWorkoutRecordCount' column\n","df = df.withColumn('PerWorkoutRecordCount', size(col('timestamp')))\n","\n","\n","# This part is writen as a function to be used again later\n","def user_activity_workout_summarize(df):\n","    user_count = format(df.select('userId').distinct().count(), ',d')\n","    workout_count = format(df.select('id').distinct().count(), ',d')\n","    activity_count = str(df.select('sport').distinct().count())\n","    sum_temp = df.agg(functions.sum('PerWorkoutRecordCount')).toPandas()\n","    total_records_count = format(sum_temp['sum(PerWorkoutRecordCount)'][0],',d')\n","    columns=['Users count', 'Activity types count','Workouts count', 'Total records count']\n","    data = [[user_count], [activity_count], [workout_count], [total_records_count]]\n","    sum_dict = {column: data[i] for i, column in enumerate(columns)}\n","    sum_df = pd.DataFrame.from_dict(sum_dict)[columns]\n","    gender_user_count = df.select('gender','userId').distinct().groupBy('gender').count().toPandas()\n","    gender_activities_count = df.groupBy('gender').count().toPandas()\n","    gender_user_activity_count = gender_user_count.join(\n","        gender_activities_count.set_index('gender'), on='gender'\n","        , how='inner', lsuffix='_gu'\n","    )\n","    gender_user_activity_count.columns = ['Gender', '# of users', 'Activities (workouts) count']\n","    \n","    return sum_df, gender_user_activity_count\n","\n","sum_dfs= user_activity_workout_summarize(df)\n","print('\\nOverall data set summary on users, workouts and number of records (pre-filtering):')\n","sum_dfs[0]"]},{"cell_type":"markdown","metadata":{"id":"dB8YnidFE7Ve"},"source":[" As we can see, The total records for this dataset is over 111M records. It's really large size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLpX0AFfE7Ve"},"outputs":[],"source":["print('Number of workouts that have less than 50 records and statistic summary:')\n","removed_df = df.select('PerWorkoutRecordCount').where(df.PerWorkoutRecordCount < 50) \\\n","               .toPandas().describe().astype(int)\n","removed_df.rename(columns = {'PerWorkoutRecordCount': 'PerWorkoutRecordCount <50'}, inplace=True)\n","removed_df.T"]},{"cell_type":"markdown","metadata":{"id":"c-5yku33E7Ve"},"source":["# **4. Pyspark lazy evaluation** <a class=\"anchor\" id=\"4\"></a>\n","\n","[Go back to table of contents](#0.1)\n","\n","Here we will begin to be familiar with some of Advanced Spark feature: **Lazy evaluation**.\n","\n","<font color=\"red\"><b>Lazy evaluation</b></font> enhances the power of Apache Spark by reducing the execution time of the RDD operations. It maintains the lineage graph to remember the operations on RDD. we can simply remember that all processing in Pyspark is abstraction, When we want to return the results, actually we tell Spark what is the eventual answer you're interested and it figures out best way to get there. As a result, it optimizes the performance and achieves fault tolerance. \n","\n","In order to see the result, we have to call Spark.collect(). \n","\n","Normolly, we can show the results with the syntax: df.take(k) or df.limit(k) to get the results with k row. \n","\n","When K become large number, These 2 way above takes a long time to complete the process. Because this syntax above did not utilize the power of Pyspark processing (Lazy evaluation).\n","In order to quickly processing , We should use df.collect()[:k] to return the k row as we want.\n","\n","You can read more about Pyspark lazy evaluation in : https://data-flair.training/blogs/apache-spark-lazy-evaluation/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DF685g6aE7Ve"},"outputs":[],"source":["ranked_sport_users_df = df.select(df.sport, df.userId) \\\n","    .distinct() \\\n","    .groupBy(df.sport) \\\n","    .count() \\\n","    .orderBy(\"count\", ascending=False)\n","\n","# Top 5 workout types\n","highest_sport_users_df = ranked_sport_users_df.limit(5).toPandas()\n","# Rename column name : 'count' --> Users count\n","highest_sport_users_df.rename(columns = {'count':'Users count'}, inplace = True)\n","# Caculate the total users, we will this result to compute percentage later\n","total_sports_users = ranked_sport_users_df.groupBy().sum().collect()[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8sjSGbBCE7Ve"},"outputs":[],"source":["ranked_sport_users_df.collect()[:5]"]},{"cell_type":"markdown","metadata":{"id":"h9Y8_G5LE7Ve"},"source":["# **5. Explolatory Data analysis** <a class=\"anchor\" id=\"5\"></a>\n","\n","[Go back to table of contents](#0.1)\n","\n","Let first do some chart indicate the top 5 workout types as we evaluated above"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6CEDrSyE7Ve"},"outputs":[],"source":["highest_sport_users_df_renamed = highest_sport_users_df\n","# Compute the percentage of top 5 workout type / total users\n","highest_sport_users_df_renamed['percentage'] = highest_sport_users_df['Users count'] \\\n","    / total_sports_users * 100\n","\n","# We assign the rest of users belong to another specific group that we call 'others'\n","others = {\n","      'sport': 'others'\n","    , 'Users count': total_sports_users - sum(highest_sport_users_df_renamed['Users count'])\n","    , 'percentage': 100 - sum(highest_sport_users_df_renamed['percentage'])\n","}\n","\n","highest_sport_users_df_renamed = highest_sport_users_df_renamed.append(\n","    others, ignore_index=True\n",")\n","print('Top 5 sports that have the most users participated:')\n","highest_sport_users_df_renamed\n","\n","fig, axs = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(0.35))\n","\n","plot0 =   axs[0].bar(x=highest_sport_users_df_renamed['sport']\n","                     , height=highest_sport_users_df_renamed['Users count'])\n","title0 =  axs[0].set_title('Users count', fontsize = 'small')\n","xlabel0 = axs[0].set_xlabel('Sport', fontsize = 'small')\n","ylabel0 = axs[0].set_ylabel('Users count', fontsize = 'small')\n","xsticks_label = axs[0].set_xticklabels(highest_sport_users_df_renamed['sport'] \n","                                       ,rotation = 'vertical', fontsize='small')\n","explode = (0.1, 0.1, 0.3, 0.3, 0.3, 0.1)\n","title1 = axs[1].set_title('User ratio', fontsize = 'small')\n","plot1 = axs[1].pie(\n","      x=highest_sport_users_df_renamed['percentage']\n","    , labels=highest_sport_users_df_renamed['sport']\n","    , autopct='%1.1f%%', shadow=True, explode=explode, startangle=90\n","    , radius=1\n",")\n","\n","text = fig.text(0.5, 1.02, 'Top 5 sports having the most users', ha='center', va='top', transform=fig.transFigure)"]},{"cell_type":"markdown","metadata":{"id":"pEDtD8M0E7Ve"},"source":["The data shows that running, walking and biking-related activities are the most spent by users, which is quite reasonable due to those exercises' convenience without much investment :3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fH8E5JT9E7Ve"},"outputs":[],"source":["# Let quick overview activities by gender\n","# we have something like this\n","activities_by_gender = df.groupBy('sport', 'gender').count().toPandas() \n","activities_by_gender[:5]"]},{"cell_type":"markdown","metadata":{"id":"CjMTLtfSE7Vf"},"source":["# **6. UNSTACK PYSPARK DATAFRAME** <a class=\"anchor\" id=\"5\"></a>\n","[Go back to table of contents](#0.1)"]},{"cell_type":"markdown","metadata":{"id":"zPCZsV6gE7Vf"},"source":["We want reshape the table above to flatten the gender column so that we can visualize on it. I draw a simple draft as follow \n","![Unstack%20dataframe.JPG](attachment:Unstack%20dataframe.JPG)\n","\n","To reshape the table like this in Pyspark, we use \n","```python\n","df.unstack()\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTGUrSAPE7Vf"},"outputs":[],"source":["total_activities = ranked_sport_users_df.count()\n","print(f'There are total: {total_activities} activities and here is the chart for activities based on gender:')\n","# Add the infor of activities based on gender\n","activities_by_gender = df.groupBy('sport', 'gender').count().toPandas()\n","# Visualize\n","fig = plt.figure(figsize=(12, 25))\n","grid_size = (1,1);\n","ax = plt.subplot2grid(grid_size, (0,0), colspan=1, rowspan=1)\n","plot = activities_by_gender.groupby(['sport', 'gender']).agg(np.mean).groupby(level=0).apply(\n","    lambda x: 100 * x / x.sum()).unstack().plot(kind='barh', stacked=True, width=1  ## APPLY UNSTACK TO RESHAPE DATA\n","                , edgecolor='black', ax=ax, title='List of all activities by gender')\n","ylabel = plt.ylabel('Sport (Activity)');\n","xlabel = plt.xlabel('Participation percentage by gender');\n","legend = plt.legend(\n","    sorted(activities_by_gender['gender'].unique()), loc='center left', bbox_to_anchor=(1.0, 0.5)\n",")\n","param_update = plt.rcParams.update({'font.size': 16});\n","ax = plt.gca()\n","formatter = ax.xaxis.set_major_formatter(mtick.PercentFormatter());\n","a = fig.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"CPldnFGiE7Vf"},"source":["Let's look at the top pareto of 5 sports that have the most participation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOU4x0DJE7Vf"},"outputs":[],"source":["activities_by_gender_df = activities_by_gender.pivot_table(\n","    index=\"sport\", columns=\"gender\", values='count', fill_value=0) \\\n","    .reset_index().rename_axis(None, axis=1)\n","\n","activities_by_gender_df['total'] = activities_by_gender_df['male'] \\\n","        + activities_by_gender_df['female'] \\\n","        + activities_by_gender_df['unknown']\n","activities_by_gender_df['percentage'] = activities_by_gender_df['total'] \\\n","    / sum(activities_by_gender_df['total']) * 100\n","top_activities_by_gender_df = activities_by_gender_df.sort_values(\n","    by='percentage', ascending=False\n",").head(5)\n","\n","others = {'sport' : 'others'}\n","for column in ['female', 'male', 'unknown', 'total', 'percentage']:\n","    value = sum(activities_by_gender_df[column]) - sum(top_activities_by_gender_df[column])\n","    others.update({column: value})\n","top_activities_by_gender_df = top_activities_by_gender_df.append(others, ignore_index=True)\n","top_activities_by_gender_df = top_activities_by_gender_df.sort_values(\n","    by='percentage', ascending=False\n",")\n","top_activities_by_gender_df\n","\n","fig, axs = plt.subplots(nrows=1, ncols=2, figsize=plt.figaspect(0.35))\n","\n","plot0 = axs[0].bar(x=top_activities_by_gender_df['sport']\n","                   , height=top_activities_by_gender_df['total'])\n","title0 = axs[0].set_title('Workout count', fontsize='small')\n","xlabel0 = axs[0].set_xlabel('Sport', fontsize='small')\n","ylabel0 = axs[0].set_ylabel('Workout count (times)', fontsize='small')\n","xsticks_label = axs[0].set_xticklabels(top_activities_by_gender_df['sport']\n","                                       , rotation='vertical', fontsize='small')\n","explode = (0.1, 0.1, 0.3, 0.3, 0.3, 0.3)\n","title1 = axs[1].set_title('Workout ratio', fontsize = 'small')\n","plot1 = axs[1].pie(\n","    x=top_activities_by_gender_df['percentage']\n","    , labels=top_activities_by_gender_df['sport']\n","    , autopct='%1.1f%%', shadow=True, explode=explode, radius=1\n",")\n","\n","text = fig.text(0.5, 1.02, 'Top 5 sports that were most participated'\n","                , ha='center', va='top', transform=fig.transFigure)"]},{"cell_type":"markdown","metadata":{"id":"kE2Wc9TWE7Vf"},"source":["Once again, similar to the user participation, running, walking and biking are also the dominant contribution interm of number of workout counts. However, the only different is that pure running and biking activities count is much bigger than those of the remaining sports, and the total count of those 2 already take up to more than 85% of total activities. \n","\n","Let play with some question such as how many people participated in more than 1 sport."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTnbAJ9NE7Vf"},"outputs":[],"source":["min_number_of_sports = 1\n","\n","sport_df = df \\\n","    .select(df.userId, df.gender, df.sport) \\\n","    .distinct() \\\n","    .groupBy(df.userId, df.gender) \\\n","    .count()    \n","\n","user_more_sports_df = sport_df \\\n","                    .filter(sport_df[\"count\"] > min_number_of_sports) \\\n","                    .orderBy(\"count\", ascending = False) \\\n","                    .toPandas()\n","user_more_sports_df.rename(columns = {'count':'Sports count'}, inplace = True)\n","user_more_sports_df.describe().astype(int).T"]},{"cell_type":"markdown","metadata":{"id":"HqoAUcJzE7Vf"},"source":["*Based on the summary, there are 822 persons participated more than 1 sport. Among them in average a person take part in about 3 sports and there is some person playing up to 16 sports!* <br />\n","Now we look at the statistic by gender in box plot:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9q61BmmbE7Vf"},"outputs":[],"source":["plot = user_more_sports_df.boxplot(column='Sports count', by='gender', fontsize='small', figsize=(6,7))"]},{"cell_type":"markdown","metadata":{"id":"qCviylLbE7Vf"},"source":["*The boxplot showed that except the outliers, both males and females have nearly the same distribution of sport participation.*"]},{"cell_type":"markdown","metadata":{"id":"nr-0ckAHE7Vf"},"source":["### Distribution of records count per workout\n","\n","*For a more detailed observation, we break down the record count per activity into each individual sport. <br />Based on the distribution, the maximum records per workout is 500, but not all workouts and sport types reach that number.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCzq3WO6E7Vg"},"outputs":[],"source":["print('\\nPlot of workouts distribution by activity type:')\n","plot_size_x, plot_size_y = 5, 5\n","figsize_x, figsize_y = plot_size_x * 4 + 3, plot_size_y * 13 + 1\n","figsize=(figsize_x, figsize_y)\n","fig = plt.figure(figsize=figsize) #\n","grid_size = (13,4)\n","ax = plt.subplot2grid(grid_size, (0,0), colspan=1, rowspan=1)\n","#fig, ax = plt.subplots()\n","PerWorkoutRecordCount_dist = df.select('PerWorkoutRecordCount', 'sport').toPandas().hist(\n","    column='PerWorkoutRecordCount', bins=10, sharex = False, grid=True\n","    , xlabelsize='small', ylabelsize='small', by='sport', ax = ax\n","    , layout = grid_size, figsize=figsize\n","    )\n","a = fig.tight_layout()\n","title = fig.text(0.5, 1, 'Distribution of records count per workout by sport', ha='center' \n","         , fontsize='small', transform=fig.transFigure);\n","xlabel = fig.text(\n","    0.5, 0.01, '# of records/workout', va='bottom', ha='center', transform=fig.transFigure\n",")\n","ylabel = fig.text(0.01, 0.5, 'Frequency (count)', va='center', rotation='vertical');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pfs6ihrCE7Vg"},"outputs":[],"source":["# Filter df with at least 10 records (as we are assumming if any user_id with less then 10 record would not be meaningful)\n","qualified_df = df \\\n","    .select(df.sport, df.userId, df.gender) \\\n","    .groupBy(df.sport, df.userId, df.gender) \\\n","    .count()\n","qualified_df = qualified_df.filter(qualified_df[\"count\"] >= 10) \\\n","    .orderBy(\"count\", ascending = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"xnXzX2u3E7Vg"},"outputs":[],"source":["print('Number of users having more than 10 workouts:')\n","qualified_pd_df = qualified_df.select(\"userId\", \"gender\").distinct() \\\n","    .groupBy(qualified_df.gender).count().toPandas()\n","qualified_pd_df.rename(columns={'count': 'Users count'}, inplace=True)\n","qualified_pd_df\n","qualified_users_count = sum(qualified_pd_df['Users count'])\n","total_users_count = df.select('userId').distinct().count()\n","qualified_percentage = round((qualified_users_count / total_users_count),2) * 100\n","print('\\nSo there is {} / {} of users qualifying the 10 historical records criteria, which is {:.2f}%' \\\n","      .format(qualified_users_count, total_users_count, qualified_percentage)\n","     )"]},{"cell_type":"markdown","metadata":{"id":"FY-9VYG7E7Vg"},"source":["# **7. Pyspark UDF Registering** <a class=\"anchor\" id=\"6\"></a>\n","[Go back to table of contents](#0.1)\n","\n","\n","To be in short, Registering UDF to Pyspark is the process of **turning Python Functions into PySpark Functions (UDF)**\n","\n","![Registering%20UDF%20to%20Pypark.JPG](attachment:Registering%20UDF%20to%20Pypark.JPG)\n","\n","When we run the code on Spark clusters, this technique will speed up the process and save valuable executed time.\n","\n","To learn more about Pyspark UDF, you can visit https://changhsinlee.com/pyspark-udf/\n","\n","Okie now we will start to apply registering python function to UDF in the 'timestamp' column\n","\n"," \n","This column is very important if we use this dataset to predict something, like predict heart rate to soonly detect some bad signal on user's heart. Because this is a type of time series analysis, so we will look at the `timestamp` column carefully.\n","### Creating some new features from `timestamp`\n","As seen before `timestamp` column contains records of timestamp series of a single workout (a data row) and is stored in UNIX timestamp format. To have more insights on this column, we will create 4 more new columns from it:  \n","- `date_time`: Convert UNIX timestamp into python's datetime format   \n","- `duration`: Total time of a single workout, in minute    \n","  *In order to get the workout `duration`, we get the difference between max and min of the datetime list of each workout.*\n","- `workout_start_time`: Determine when, which hour of the day a workout start  \n","  *For `workout_start_time`, it's the hour part of the first datetime record of a workout.*\n","- `interval`: List of time lapses between each single timestamp record in a single workout, in second   \n","  *And for `interval`, we will calculate it by taking the difference between 2 consecutive timestamp records within a workout.*\n","\n","In order to attach these 4 features to SparkdDataFrame, we register them with PYSPARK UDF \n","\n","```python\n","function_to_udf = udf(function, Datatype()) \n","\n","# Datatype() can be floattype(), TimestampType(), etc\n","\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwpWxnaXE7Vg"},"outputs":[],"source":["# LOOK AGAIN THE TIMESTAMP COLUMN\n","df.limit(3).toPandas()"]},{"cell_type":"markdown","metadata":{"id":"e2Pfhn_TE7Vg"},"source":["### We create 4 helper function for 'timestamp' column as described above then convert them to UDF"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"wabc8mFqE7Vg"},"outputs":[],"source":["# Convert a timestamp column into Datetime.Datetime, to be used for .withColumn function later\n","def to_time(timestamp_list):\n","    # convert to datetime and minus 7 hours due to the difference in Endomondo time window with utc time as the dataset description\n","    return [datetime.fromtimestamp(t) - timedelta(hours=7) for t in timestamp_list]\n","\n","# Register 'to_time' function into UDF pyspark framework\n","udf_to_time = udf(to_time, ArrayType(elementType=TimestampType()))\n","\n","# Support function to get the duration (in minutes) of a list of datetime values, to be used for withColumn function later\n","def get_duration(datetime_list):\n","    time_dif = max(datetime_list) - min(datetime_list)\n","    return time_dif.seconds/60\n","\n","# Register the support function 'get_duration' as a user defined function into pyspark framework\n","udf_get_duration = udf(get_duration, FloatType())\n","\n","# Support function to get the workout start time of the datetime list, to be used for withColumn function later\n","def get_start_time(datetime_list):\n","    return min(datetime_list)\n","\n","# Register the support function 'get_start_time' as a user defined function into pyspark framework\n","udf_get_start_time = udf(get_start_time, TimestampType())\n","\n","# Support function to get list of intervals within a workout\n","def get_interval(datetime_list):\n","    if len(datetime_list) == 1:\n","        return [0]\n","    else:\n","        interval_list = []\n","        for i in range(0, len(datetime_list)-1):\n","            interval = (datetime_list[i+1] - datetime_list[i]).seconds\n","            interval_list.append(interval)\n","        return interval_list\n","\n","# Register the support function 'get_interval' as a user defined function into pyspark framework    \n","udf_get_interval = udf(get_interval, ArrayType(elementType=IntegerType()))\n","\n","# Create new 'date_time' column to convert from timestamp into python's datetime format for later usage\n","df = df.withColumn('date_time', \n","    udf_to_time('timestamp'))\n","\n","# Create 'workout_start_time' column to get the start time of each workout/row:\n","df = df.withColumn('workout_start_time', hour(udf_get_start_time('date_time')))\n","\n","# Create duration column from the date_time column just created, using the udf function udf_get_duration defined above\n","df = df.withColumn('duration', udf_get_duration('date_time'))\n","\n","# Create interval column from the date_time column, using the udf function udf_get_interval defined above\n","df = df.withColumn('interval', udf_get_interval('date_time'))\n","\n","print('New columns (''date_time'', ''workout_start_time'' in hour\\\n",", ''duration'' in minutes & ''interval'' in seconds)\\n, first 5 rows:')\n","df.select('timestamp','date_time', 'workout_start_time', 'duration', 'interval').limit(5).toPandas()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ijhmYbSXE7Vg"},"source":["### Now, we look at the duration of each workout (in minutes). First is some typical statistics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OL-CtwRqE7Vg"},"outputs":[],"source":["print('\\nLet''s look at the statistics of the new duration column (in minutes):')\n","df.select('duration').toPandas().describe().T"]},{"cell_type":"markdown","metadata":{"id":"w8RW_8rPE7Vg"},"source":["From the statistic of `duration` column, it can be observed that workout duration can last from 0 minute and up to 1 full day (1440 minutes = 24 hours). The duration of 0 might be for workouts that only have 1 single record only, so the min and the max timestamp would be the same.<br />\n","Now it's plot time for duration:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOMUJaEeE7Vg"},"outputs":[],"source":["\n","print('\\nPlotting distribution of duration per sport type:')\n","plot_size_x, plot_size_y = 5, 5\n","figsize_x, figsize_y = plot_size_x * 4 +3, plot_size_y * 13 + 1\n","figsize = (figsize_x, figsize_y)\n","fig = plt.figure(figsize=figsize) #\n","grid_size = (13,4)\n","ax = plt.subplot2grid(grid_size, (0,0), colspan=1, rowspan=1)\n","\n","duration_dist = df.select('duration', 'sport').toPandas().hist(\n","    column='duration', by='sport', bins=15, sharex = False, grid=True\n","    , xlabelsize='small', ylabelsize='small' , ax = ax\n","    , layout = grid_size, figsize=figsize\n","    )\n","a = fig.tight_layout()\n","title = fig.text(0.5, 1, 'Distribution of workout duration by sport'\n","             , ha='center', va='center', transform=fig.transFigure\n","            )\n","xlabel = fig.text(0.5, 0.01, 'Workout duration (minutes)'\n","             , ha='center', va='center', transform=fig.transFigure)\n","ylabel = fig.text(0.01, 0.5, 'Frequency (count)', va='center', rotation='vertical');"]},{"cell_type":"markdown","metadata":{"id":"jO5mK1lhE7Vh"},"source":["*From the duration summary & distribution plot, majority of the activities happens in 1-2 hours, only a few sports with a few cases each type happened in longer durations, such as moutain bike, hiking, sailing, etc..*"]},{"cell_type":"markdown","metadata":{"id":"RH2NybagE7Vh"},"source":["# **8. Convert row objects to Spark Resilient Distributed Dataset (RDD)** <a class=\"anchor\" id=\"7\"></a>\n","[Go back to table of contents](#0.1)\n","\n","In this plot, we will practice how to convert the row object to RDD format in Pyspark through:\n","\n","```python\n","rdd = df.rdd.map(tuple)\n","or\n","rdd = df.rdd.map(list)\n","```\n","\n","The advanced of RDD format is: Each data set is divided into logical parts and these can be easily computed on different nodes of the cluster. They can be operated in parallel and are fault-tolerant, so that the process is stable and very fast\n","\n","If we run this code such as on Zeppelin which intergrated with Pyspark clusters,we can see how speed RDD spark is.\n","\n","RDD is very important concept in Spark and you can deep into more in here :\n","https://www.educba.com/what-is-rdd/\n","\n","To practice this concept, we look at the `interval` column and have some statistics for it. we will calculate some major statistics (min/max/mean/average/standar deviation and 4 quantiles 25th/50th/75th/95th) info in pySpark, convert to Rdd and plot them.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIazB5lBE7Vh"},"outputs":[],"source":["#  Helper function to calculate statistic(s) of the column name from a tuple x of (sport, records list of the column)\n","#, the stats to calculate is also given as an input\n","def calculate_stats(x,column_name, stat_list):\n","    sport, records_list = x\n","    stat_dict = {'sport': sport}\n","    if 'min' in stat_list:\n","        min_stat = min(records_list)\n","        stat_dict.update({'min ' + column_name : min_stat})\n","    if 'max' in stat_list:\n","        max_stat = max(records_list)\n","        stat_dict.update({'max ' + column_name: max_stat})\n","    if 'mean' in stat_list:\n","        average_stat = stats.mean(records_list)\n","        stat_dict.update({'mean ' + column_name: average_stat})\n","    if 'stdev' in stat_list:\n","        std_stat = stats.stdev(records_list)\n","        stat_dict.update({'stdev ' + column_name: std_stat})\n","    if '50th percentile' in stat_list:\n","        median_stat = stats.median(records_list)\n","        stat_dict.update({'50th percentile ' + column_name: median_stat})\n","    if '25th percentile' in stat_list:\n","        percentile_25th_stat = np.percentile(records_list, 25)\n","        stat_dict.update({'25th percentile ' + column_name: percentile_25th_stat})\n","    if '75th percentile' in stat_list:\n","        percentile_75th_stat = np.percentile(records_list, 75)\n","        stat_dict.update({'75th percentile ' + column_name: percentile_75th_stat})\n","    if '95th percentile' in stat_list:\n","        percentile_95th_stat = np.percentile(records_list, 95)\n","        stat_dict.update({'95th percentile ' + column_name: percentile_95th_stat})\n","    return stat_dict\n","\n","def to_list(a):\n","    return a\n","\n","def extend(a, b):\n","    a.extend(b)\n","    return a\n","\n","def retrieve_array_column_stat_df(df, column_name, stat_list):\n","    # Convert sport & \"column_name\" to RDD to easily calculate the statistics of intervals by sports\n","    sport_record_rdd = df.select('sport', column_name).rdd \\\n","    .map(tuple).combineByKey(to_list, extend, extend).persist()\n","\n","    # Calculate statistics of the input column by calling calculate_stats function defined above\n","    record_statistic_df = pd.DataFrame(sport_record_rdd.map(\n","        lambda x: calculate_stats(x, column_name,stat_list)).collect()\n","                                      )\n","    # Set proper dataframe column orders\n","    columns_order = ['sport'] + [stat + ' ' + column_name for stat in stat_list]\n","    # Re order columns\n","    return record_statistic_df[columns_order]\n","\n","stat_list = ['min', '25th percentile', 'mean', '50th percentile',\n","                     '75th percentile', '95th percentile', 'max', 'stdev']\n","interval_statistic_df = retrieve_array_column_stat_df(df, column_name='interval', stat_list=stat_list)\n","print('\\nLet\\'s look at statistic for interval, in seconds (by sport):' )\n","interval_statistic_df"]},{"cell_type":"markdown","metadata":{"id":"Gwk9GF48E7Vh"},"source":["Now we plot those numbers in bar (for quantiles statistics) and line charts (for min/max/mean/stdev) for a more visualized feel.  \n","*Note: Due to the fact that the maximum interval and stdev have a much higher order of magnitude compared to the remaining columns, we need to put those 2 columns in a separate y axis on the right.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnYroi3JE7Vh"},"outputs":[],"source":["print('\\nSummarize statistics of interval sport:')\n","bar_columns = ['25th percentile interval', '50th percentile interval'\n","               , '75th percentile interval', '95th percentile interval']\n","line_columns1 = ['min interval', 'mean interval'] \n","line_columns2 = ['max interval', 'stdev interval'] \n","interval_statistic_df = interval_statistic_df.sort_values(\n","    by='95th percentile interval', ascending=False\n",")\n","figsize=(13, 59)\n","fig, axs = plt.subplots(nrows=7, figsize=figsize)\n","\n","d = axs[0].set_title('Interval statistics by sport', fontsize=18)\n","for i in range (7):\n","    interval_statistic_sub_df = interval_statistic_df.iloc[i*7:i*7+7,]\n","    #interval_statistic_sub_df\n","    plot1 = interval_statistic_sub_df[['sport'] + bar_columns] \\\n","        .groupby(['sport']).agg(np.mean).plot(\n","        kind='bar', stacked=True, grid=False, alpha=0.5, edgecolor='black', ax=axs[i], \n","    )\n","    plot2 = interval_statistic_sub_df[['sport'] + line_columns1].plot(x='sport', ax=axs[i], marker='o')\n","    ax2 = axs[i].twinx()\n","    plot3 = interval_statistic_sub_df[['sport'] + line_columns2].plot( x='sport', ax=ax2, marker='o', color=['m', 'g'])\n","    a = axs[i].legend(loc='center left', fontsize=16, bbox_to_anchor=(1.2, 0.5), frameon=False)\n","    a = ax2.legend(  labels=['max interval (right)', 'stdev interval (right)']\n","                   , loc=\"center left\", fontsize=16, bbox_to_anchor=(1.2, 0.11), frameon=False)\n","    b = axs[i].set_xticklabels(interval_statistic_sub_df['sport'],rotation = 'horizontal', fontsize='small')\n","    c = axs[i].set_xlabel('Sport (Activity)', fontsize='small');\n","    d = axs[i].set_ylabel('Quantiles Statistics + min/mean\\n(second)', fontsize=16);\n","    e = ax2.set_ylabel('Max/stdev Statistics\\n(second)', fontsize=16)\n","    for tick in axs[i].yaxis.get_major_ticks():\n","        a = tick.label.set_fontsize(16) \n","    ax2.tick_params(axis='y', labelsize=16)\n","    b = plt.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=True)\n","\n","plt.subplots_adjust(hspace=0.2)\n","plt.show();\n"]},{"cell_type":"markdown","metadata":{"id":"tQNs61sFE7Vh"},"source":["Looking at the quantiles statistic, up to 95% of the interval data set does not have the interval larger than 400 seconds, while there are just a few outliers that made the maximum intervals reach up to 86400 seconds (a full days)."]},{"cell_type":"markdown","metadata":{"id":"74O7Z1TUE7Vh"},"source":["### Workout start time\n","Once again, we use histogram chart to look at the distribution of workouts' started hours, grouped by sport and broken down by gender. We divide a day into intervals of 2 hours, so there are totally 12 buckets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdi-IEUlE7Vh"},"outputs":[],"source":["# Retrive the table of gender, sport and workout_start_time for plotting\n","start_time_df = df.select('gender', 'sport','workout_start_time').toPandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ny2YfdGwE7Vh"},"outputs":[],"source":["activities = start_time_df['sport'].unique()\n","plot_size_x, plot_size_y = 5, 5\n","figsize_x, figsize_y = (plot_size_x + 0.5) * 4 +3, (plot_size_y + 1) * 13 + 1\n","\n","\n","nrows, ncols = 13, 4\n","a = fig.subplots_adjust(hspace = 1, wspace = 1)\n","fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figsize_x, figsize_y))\n","print('\\nPlotting distribution of workout start time per sport type, break down by gender:')\n","a = plt.setp(axs, xticks=[0, 4, 8, 12, 16, 20])\n","for index, sport in enumerate(activities):\n","    row_index, col_index = divmod(index, ncols)\n","    male_start_time_list = start_time_df[(start_time_df.sport == sport) & \n","                                            (start_time_df.gender == 'male')]['workout_start_time']\n","    female_start_time_list = start_time_df[(start_time_df.sport == sport) & \n","                                            (start_time_df.gender == 'female')]['workout_start_time']\n","    unknown_start_time_list = start_time_df[(start_time_df.sport == sport) & \n","                                            (start_time_df.gender == 'unknown')]['workout_start_time']\n","    if len(male_start_time_list) > 0:\n","        male_dist = axs[row_index, col_index].hist(male_start_time_list,\n","                                      bins = 12, alpha=0.5, label='male', range=(0, 23))\n","    if len(female_start_time_list) > 0:    \n","        female_dist = axs[row_index, col_index].hist(female_start_time_list,\n","                                      bins = 12, alpha=0.5, label='female', range=(0, 23))\n","    if len(unknown_start_time_list) > 0:\n","        unknown_dist = axs[row_index, col_index].hist(unknown_start_time_list,\n","                                      bins = 12, alpha=0.5, label = 'unknown', range=(0, 23))\n","    b= axs[row_index, col_index].set_title('Activitiy: ' + sport, fontsize='small')\n","    a = axs[row_index, col_index].legend(loc=\"upper left\", fontsize='small')\n","    a = plt.setp(axs[row_index, col_index].get_xticklabels(), fontsize='small')\n","\n","for i in range(1,4):\n","    x = axs[12, i].set_visible(False)\n","a = fig.tight_layout()\n","z = fig.text(0.5, 1, 'Distribution of workout started time (hour) by sport'\n","             , ha='center', va='top', transform=fig.transFigure)\n","y = fig.text(0.5, 0.01, 'Workout started hour in a day (hour)'\n","             , ha='center', va='bottom', transform=fig.transFigure)\n","z = fig.text(0.02, 0.5, 'Frequency (count)', va='center', rotation='vertical');"]},{"cell_type":"markdown","metadata":{"id":"wgyvqVVIE7Vh"},"source":["From the distribution charts above, it can be seen that most of the sports have activities started either in the morning or eveneing (bimodal distribution), which does make sense. There are a few activities happening during the timeframe of 0-4 o'clock, which is quite odd.  \n","<br />\n","\n","### Look deeper into row level information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxI6yB6qE7Vh"},"outputs":[],"source":["stat_list = ['min', '25th percentile', 'mean', '95th percentile', 'max', 'stdev']\n","heart_rate_statistic_df = retrieve_array_column_stat_df(df, column_name='heart_rate', stat_list=stat_list)"]},{"cell_type":"markdown","metadata":{"id":"nk37lWWrE7Vh"},"source":["*Due to the huge amount of users and workout numbers, we just picked randomly up to a x number of users per gender (ex, 5), and up to y workouts per activity type (ex, 10).<br />*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-O2Yd_GE7Vh"},"outputs":[],"source":["# Support function helping to sample data\n","def sampling_data(max_users_per_gender, max_workouts_per_sport):\n","    '''\n","        max_users_per_gender: maximum number of user to be selected randomly per gender\n","        max_workouts_per_sport: maximum number of activities to be selected per sport \n","        (the sports existing in selected users)\n","    '''\n","    # Get unique list of userId and gender, for sampling purpose\n","    users_genders = df.select('userId', 'gender').distinct().toPandas()\n","    # Use 'sample' function to pick up to 3 userId per gender from the unique userId list\n","    random_x_users_per_gender = users_genders.groupby('gender')['userId'].apply(\n","                lambda s: s.sample(min(len(s), max_users_per_gender))\n","    )\n","\n","    # Apply filter on the main pyspark dataframe for sampling\n","    samples_by_gender = df.where(df.userId.isin(list(random_x_users_per_gender)))\n","\n","    # Next, generate the unique activity ids and sport types list from the sampled data set \n","    workout_sports = samples_by_gender.select('id', 'sport').distinct().toPandas()\n","    # Use 'sample' function to pick up to 10 activity ids for each kind of sport \n","    random_y_workouts_per_sport = workout_sports.groupby('sport')['id'].apply(\n","        lambda s: s.sample(min(len(s), max_workouts_per_sport))\n","    )\n","\n","    # Apply filter to the sampled dataset to continue reduce the number of workouts per activity type\n","    samples_by_gender_and_sport = samples_by_gender.where(df.id.isin(list(random_y_workouts_per_sport)))\n","    return samples_by_gender_and_sport"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"RRIUVB79E7Vi"},"outputs":[],"source":["# Use 2 variable to determine the sampling criteria: \n","# maximum users per gender and maximum workouts per sport\n","max_users_per_gender, max_workouts_per_sport = 20, 15\n","\n","# Collect the sampled data set to Pandas to be used with plot features\n","pd_df = sampling_data(max_users_per_gender, max_workouts_per_sport).toPandas()\n","print('\\nSampled data overview (only string and numeric columns):')\n","pd_df.describe()"]},{"cell_type":"markdown","metadata":{"id":"cUftptBEE7Vi"},"source":["we will normalize the time for all workouts by calulating the duration (in seconds) of each timestamp record from the first record of a workout (the first datetime element of the list in that workout). <br />\n","Then we plot the heart rate on this normalized time, grouping by sport."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1kBcsvwE7Vi"},"outputs":[],"source":["# Lambda function to flatten a list of lists into a big single list\n","flattern = lambda l: set([item for sublist in l for item in sublist])\n","\n","normalized_datetime_list = []\n","for index,data_row in pd_df.iterrows():\n","    min_date_time = min(data_row['date_time'])\n","    normalized_datetime_list.append(\n","        [(date_time - min_date_time).seconds for date_time in data_row['date_time']]\n","    )\n","\n","pd_df['normalized_date_time'] = normalized_datetime_list\n","\n","print('New normalized datetime (first 7 rows):')\n","pd_df.head(7)[['userId', 'sport', 'date_time','normalized_date_time']]\n","\n","print('\\nPlot raw heart rate (sampled) by normalized time:')\n","\n","sport_list = pd_df['sport'].unique()\n","# Define the length of the figure dynamically depends on the length of the sport list\n","fig, axs = plt.subplots(len(sport_list), figsize=(15, 6*len(sport_list)))\n","subplot_adj = fig.subplots_adjust(hspace = 0.6)\n","plot_setp = plt.setp(axs, yticks=range(0,250,20))\n","\n","for sport_index, sport in enumerate(sport_list):\n","    workout = pd_df[pd_df.sport == sport]\n","    max_time = max(flattern(workout.normalized_date_time))\n","    for workout_index, data_row in workout.iterrows():\n","        label = 'user: ' + str(data_row['userId']) + ' - gender: ' + data_row['gender']\n","        plot_i = axs[sport_index].plot(\n","            data_row['normalized_date_time'], data_row['heart_rate'], label=label\n","        )\n","    title_i = axs[sport_index].set_title('Activitiy: ' + sport, fontsize='small')\n","    xlabel_i = axs[sport_index].set_xlabel('Time (sec)', fontsize='small')\n","    xsticklabels_i = axs[sport_index].set_xticklabels(\n","        range(0, max_time, 500),rotation = 'vertical', fontsize=9\n","    )\n","    ysticklabels_i = axs[sport_index].set_yticklabels(range(0,250,20),fontsize='small')\n","    legend_i = axs[sport_index].legend(\n","        loc='center left', bbox_to_anchor=(1.0, 0.5), prop={'size': 9}\n","    )\n","\n","x_label = fig.text(0.04, 0.5, 'Heart rate (bpm)', va='center', rotation='vertical')\n","chart_title = fig.text(0.5, 1.3, 'Raw heart rate (sample) by normalized time', \n","            ha='center', va='center', fontsize='small', transform=axs[0].transAxes)\n"]},{"cell_type":"markdown","metadata":{"id":"CvY2Yod3E7Vi"},"source":["### Workout displacements\n","We will have some visualization on 3 displacement/geometry info columns (`longitude`,`latitude` & `altitude`).    \n","Since the geometry location of each user and workout is different from each others, we only plot a few single workouts in 3D plots to have a look on the workout route.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPW4p6jlE7Vi"},"outputs":[],"source":["pd_df_small = sampling_data(max_users_per_gender=2, max_workouts_per_sport=2).toPandas()\n","print('Sampled data (2 user, 2 workouts per sport):')\n","pd_df_small[['userId', 'gender','sport','id', 'workout_start_time'\n","             ,'PerWorkoutRecordCount', 'duration', 'longitude', 'latitude', 'altitude']].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"id":"o4YPV0TVE7Vi"},"outputs":[],"source":["def get_fixed_mins_maxs(mins, maxs):\n","    deltas = (maxs - mins) / 12.\n","    mins = mins + deltas / 4.\n","    maxs = maxs - deltas / 4.\n","\n","    return [mins, maxs]\n","\n","workout_count = pd_df_small.shape[0]\n","ncols = 3\n","nrows = math.ceil(workout_count/ncols)\n","#workout_count\n","fig = plt.figure(figsize=(8 * (ncols + 0.5), 8*nrows))\n","\n","a = fig.subplots_adjust(hspace = 0.2, wspace=0.5)\n","#c = plt.setp(axs, yticks=range(0,250,20))\n","\n","print('Plot workout path in 3D graphs per each workout:')\n","for row_index, row in pd_df_small.iterrows():\n","    if row_index==2:\n","        text = ax.text2D(\n","            0.01, 1, \"Workout path (longitude/latitude/altitude)\"\n","            , fontsize=18, transform=ax.transAxes\n","        )\n","    min_long = min(row['longitude']) - stats.stdev(row['longitude'])\n","    max_long = max(row['longitude']) + stats.stdev(row['longitude'])\n","    minmax_long = get_fixed_mins_maxs(min_long, max_long)\n","    #minmax_long\n","    min_lat = min(row['latitude']) - stats.stdev(row['latitude'])\n","    max_lat = max(row['latitude']) + stats.stdev(row['latitude'])\n","    minmax_lat = get_fixed_mins_maxs(min_lat, max_lat)\n","    #minmax_lat\n","    min_alt = min(row['altitude']) - stats.stdev(row['altitude'])\n","    max_alt = max(row['altitude']) + stats.stdev(row['altitude'])\n","    minmax_alt = get_fixed_mins_maxs(min_alt, max_alt)\n","    #minmax_alt\n","    ax = fig.add_subplot(nrows, ncols, row_index + 1, projection='3d')\n","    title = 'Activitiy: ' + row['sport'] + ' - Gender: ' + row['gender'] \\\n","        + '\\nRecords: ' + str(int(row['PerWorkoutRecordCount'])) \\\n","        + ' - Duration: ' + str(int(row['duration'])) + ' minutes'\n","    title = ax.set_title(title, fontsize=16)\n","    scatter = ax.scatter(row['longitude'], row['latitude'], row['altitude'], c='r', marker='o')\n","    plot = ax.plot3D(\n","        row['longitude'], row['latitude'], row['altitude'], c='gray', label='Workout path'\n","    )\n","    \n","    x_label = ax.set_xlabel('Longitude (Degree)', fontsize=16)\n","    y_label = ax.set_ylabel('Latitude (Degree)', fontsize=16)\n","    z_label = ax.set_zlabel('Altitude (m)', fontsize=16, rotation = 0)\n","    for t in ax.xaxis.get_major_ticks():\n","        font_size = t.label.set_fontsize(16)\n","    for t in ax.yaxis.get_major_ticks():\n","        font_size = t.label.set_fontsize(16)\n","    for t in ax.zaxis.get_major_ticks():\n","        font_size = t.label.set_fontsize(16)\n","    legend = ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n","    ax.zaxis.set_rotate_label(False)\n","    #b = plt.setp(ax.get_xticklabels(), rotation=41)\n","    #b = plt.setp(ax.get_yticklabels(), rotation=-30)\n","    plt.gca().xaxis.set_major_formatter(mtick.FormatStrFormatter('%.3f'))\n","    plt.gca().yaxis.set_major_formatter(mtick.FormatStrFormatter('%.3f'))\n","    ax.pbaspect = [4, 2, 0.5]\n","    xlims = ax.set_xlim(minmax_long)\n","    ylims = ax.set_ylim(minmax_lat)\n","    # Some sports will not have altitude change so check it before set z limit\n","    if minmax_alt[0] != minmax_alt[1]: zlims = ax.set_zlim(minmax_alt)\n","    # Do this trick to enable tight_layout for 3D plot:\n","    for spine in ax.spines.values():\n","        b = spine.set_visible(False)\n","plt.rcParams['legend.fontsize'] = 16\n","a = plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VSvdsKhFE7Vi"},"source":["Thank you for reading my work, wish you strong and stay safe"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}