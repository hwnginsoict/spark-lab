{"cells":[{"cell_type":"markdown","metadata":{"id":"HbLLaWeY6fW8"},"source":["Chapter 5 presented basic DataFrame concepts and abstractions. This chapter covers building\n","expressions, which are the bread and butter of Spark’s structured operations. We also review\n","working with a variety of different kinds of data, including the following:\n","1. Booleans\n","2. Numbers\n","3. Strings\n","4. Dates and timestamps\n","5. Handling null\n","6. Complex types\n","7. User-defined functions\n","\n","To begin, let’s read in the DataFrame that we’ll be using\n","for this analysis:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpoTuQlS6fW_"},"outputs":[],"source":["from pyspark.context import SparkContext\n","from pyspark.sql.session import SparkSession\n","sc = SparkContext('local')\n","spark = SparkSession(sc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7dGskDr6fW_","outputId":"dacf4534-6753-441f-ff5d-1818986ded92"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- InvoiceNo: string (nullable = true)\n"," |-- StockCode: string (nullable = true)\n"," |-- Description: string (nullable = true)\n"," |-- Quantity: integer (nullable = true)\n"," |-- InvoiceDate: timestamp (nullable = true)\n"," |-- UnitPrice: double (nullable = true)\n"," |-- CustomerID: double (nullable = true)\n"," |-- Country: string (nullable = true)\n","\n"]}],"source":["df = spark.read.format(\"csv\")\\\n",".option(\"header\", \"true\")\\\n",".option(\"inferSchema\", \"true\")\\\n",".load(\"../data/retail-data/by-day/2010-12-01.csv\")\n","df.printSchema()\n","df.createOrReplaceTempView(\"dfTable\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wROeX3ea6fW_","outputId":"904d1abd-70a5-413d-a69c-969f49e422bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n","|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n","+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n","|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n","|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n","+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n","only showing top 2 rows\n","\n"]}],"source":["df.show(2)"]},{"cell_type":"markdown","metadata":{"id":"UDeAEcmG6fXA"},"source":["# Converting to Spark Types\n","One thing you’ll see us do throughout this chapter is convert native types to Spark types. We do\n","this by using the first function that we introduce here, the lit function. This function converts a\n","type in another language to its correspnding Spark representation. Here’s how we can convert a\n","couple of different kinds of Scala and Python values to their respective Spark types:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqIuZK116fXA","outputId":"0bdf6365-7108-40d1-8eff-ad426c551883"},"outputs":[{"data":{"text/plain":["DataFrame[5: int, five: string, 5.0: double]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from pyspark.sql.functions import lit\n","df.select(lit(5), lit(\"five\"), lit(5.0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NaiVjm_6fXA"},"outputs":[],"source":["#There’s no equivalent function necessary in SQL, so we can use the values directly:\n","#-- in SQL\n","#SELECT 5, \"five\", 5.0"]},{"cell_type":"markdown","metadata":{"id":"x5CjxN0L6fXA"},"source":["# Working with Booleans\n","Booleans are essential when it comes to data analysis because they are the foundation for all\n","filtering. Boolean statements consist of four elements: and, or, true, and false. We use these\n","simple structures to build logical statements that evaluate to either true or false. These statements\n","are often used as conditional requirements for when a row of data must either pass the test\n","(evaluate to true) or else it will be filtered out.\n","\n","Let’s use our retail dataset to explore working with Booleans. We can specify equality as well as\n","less-than or greater-than:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKIXcUn06fXA","outputId":"8d6219fa-aacf-4b51-ba2e-3f45ef14311d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+-----------------------------+\n","|InvoiceNo|Description                  |\n","+---------+-----------------------------+\n","|536366   |HAND WARMER UNION JACK       |\n","|536366   |HAND WARMER RED POLKA DOT    |\n","|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n","|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n","|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n","+---------+-----------------------------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.sql.functions import col\n","df.where(col(\"InvoiceNo\") != 536365)\\\n",".select(\"InvoiceNo\", \"Description\")\\\n",".show(5, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Xb5dyrW6fXA","outputId":"1a0312c2-6ca3-458c-d674-3303cd5948a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+--------------------+\n","|InvoiceNo|         Description|\n","+---------+--------------------+\n","|   536366|HAND WARMER UNION...|\n","|   536366|HAND WARMER RED P...|\n","|   536367|ASSORTED COLOUR B...|\n","|   536367|POPPY'S PLAYHOUSE...|\n","|   536367|POPPY'S PLAYHOUSE...|\n","+---------+--------------------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.sql.functions import col\n","df.where(col(\"InvoiceNo\") != 536365)\\\n",".select(\"InvoiceNo\", \"Description\")\\\n",".show(5, True)"]},{"cell_type":"markdown","metadata":{"id":"Qx3-TNT76fXA"},"source":["Another option—and probably the cleanest—is to specify the predicate as an expression in a\n","string. This is valid for Python or Scala. Note that this also gives you access to another way of\n","expressing “does not equal”:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5pj_PGk6fXA","outputId":"1211f981-b6a0-4624-8d84-62ad54fb96d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n","|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n","+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n","|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n","|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n","|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n","|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n","|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n","+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n","only showing top 5 rows\n","\n","+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n","|InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n","+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n","|536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n","|536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n","|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|\n","|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n","|536367   |22748    |POPPY'S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n","+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.where(\"InvoiceNo = 536365\").show(5, False)\n","df.where(\"InvoiceNo <> 536365\").show(5, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enm7zWkx6fXB","outputId":"2ef0ee06-554f-49ee-a214-c01cc95b6c5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n","|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n","+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n","|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n","|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n","|   536372|    22632|HAND WARMER RED P...|       6|2010-12-01 09:01:00|     1.85|   17850.0|United Kingdom|\n","|   536372|    22633|HAND WARMER UNION...|       6|2010-12-01 09:01:00|     1.85|   17850.0|United Kingdom|\n","|   536377|    22632|HAND WARMER RED P...|       6|2010-12-01 09:34:00|     1.85|   17850.0|United Kingdom|\n","+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.where(\"Description like 'HAND WARMER%'\").show(5, True)"]},{"cell_type":"markdown","metadata":{"id":"erSfQBh26fXB"},"source":["We mentioned that you can specify Boolean expressions with multiple parts when you use and\n","or or. In Spark, you should always chain together and filters as a sequential filter.\n","\n","The reason for this is that even if Boolean statements are expressed serially (one after the other),\n","Spark will flatten all of these filters into one statement and perform the filter at the same time,\n","creating the and statement for us. Although you can specify your statements explicitly by using\n","and if you like, they’re often easier to understand and to read if you specify them serially. or\n","statements need to be specified in the same statement:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLQDm9eM6fXB","outputId":"442242e2-da7a-4de0-a070-ebf097d16abf"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n","|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n","+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n","|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n","|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n","+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import instr\n","priceFilter = col(\"UnitPrice\") > 600\n","descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n","df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()\n","#-- in SQL\n","#SELECT * FROM dfTable WHERE StockCode in (\"DOT\") AND(UnitPrice > 600 OR\n","#instr(Description, \"POSTAGE\") >= 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFUXDWOM6fXB","outputId":"9475f221-ea29-4381-f165-a2262382a529"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n","|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n","+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n","|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n","|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n","+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n","\n"]}],"source":["spark.sql('SELECT * FROM dfTable WHERE StockCode in (\"DOT\") \\\n","          AND(UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1)').show(2)"]},{"cell_type":"markdown","metadata":{"id":"1AveilPG6fXB"},"source":["Boolean expressions are not just reserved to filters. To filter a DataFrame, you can also just\n","specify a Boolean column:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeqXXCzC6fXB","outputId":"55c26a70-62c3-4514-fc45-465cde8b2121"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+-----------+\n","|unitPrice|isExpensive|\n","+---------+-----------+\n","|   569.77|       true|\n","|   607.49|       true|\n","+---------+-----------+\n","\n"]}],"source":["from pyspark.sql.functions import instr\n","DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n","priceFilter = col(\"UnitPrice\") > 600\n","descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n","df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",".where(\"isExpensive\")\\\n",".select(\"unitPrice\", \"isExpensive\").show(5)\n","#-- in SQL\n","#SELECT UnitPrice, (StockCode = 'DOT' AND\n","#(UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1)) as isExpensive\n","#FROM dfTable\n","#WHERE (StockCode = 'DOT' AND\n","#(UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1))"]},{"cell_type":"markdown","metadata":{"id":"VZRUo_FW6fXB"},"source":["Notice how we did not need to specify our filter as an expression and how we could use a\n","column name without any extra work.\n","\n","If you’re coming from a SQL background, all of these statements should seem quite familiar.\n","Indeed, all of them can be expressed as a where clause. In fact, it’s often easier to just express\n","filters as SQL statements than using the programmatic DataFrame interface and Spark SQL\n","allows us to do this without paying any performance penalty. For example, the following two\n","statements are equivalent:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9DNuAeU6fXB","outputId":"1f50518e-b9cb-4390-cbe1-ceb01fb06149"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+---------+-----------+\n","|   Description|UnitPrice|isExpensive|\n","+--------------+---------+-----------+\n","|DOTCOM POSTAGE|   569.77|       true|\n","|DOTCOM POSTAGE|   607.49|       true|\n","+--------------+---------+-----------+\n","\n"]}],"source":["from pyspark.sql.functions import expr\n","df2 = df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n",".where(\"isExpensive\")\\\n",".select(\"Description\", \"UnitPrice\", 'isExpensive')\n","df2.show()"]},{"cell_type":"markdown","metadata":{"id":"M3YVvEco6fXB"},"source":["## WARNING\n","One “gotcha” that can come up is if you’re working with null data when creating Boolean expressions.\n","If there is a null in your data, you’ll need to treat things a bit differently. Here’s how you can ensure\n","that you perform a null-safe equivalence test:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZBW20y7U6fXB","outputId":"3ba9e8a2-5413-4ecc-ea99-52256216cce8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+-----------+--------+-----------+---------+----------+-------+\n","|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n","+---------+---------+-----------+--------+-----------+---------+----------+-------+\n","+---------+---------+-----------+--------+-----------+---------+----------+-------+\n","\n"]}],"source":["df.where(col(\"Description\").eqNullSafe(\"hello\")).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQSwXSxv6fXB","outputId":"98a3009f-588f-4a8f-aece-ff83e8767b64"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+-----------+--------+-----------+---------+----------+-------+\n","|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n","+---------+---------+-----------+--------+-----------+---------+----------+-------+\n","+---------+---------+-----------+--------+-----------+---------+----------+-------+\n","\n"]}],"source":["df.where(col(\"Description\") == \"hello\").show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57iaHEEx6fXB","outputId":"3600846d-a980-4c4e-e258-e9d4150cacaa"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n","|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n","+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n","|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n","|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n","+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n","\n"]}],"source":["\n","df.where(col(\"Description\").eqNullSafe(\"DOTCOM POSTAGE\")).show()"]},{"cell_type":"markdown","metadata":{"id":"Io88cyD06fXB"},"source":["# Working with Numbers\n","When working with big data, the second most common task you will do after filtering things is\n","counting things. For the most part, we simply need to express our computation, and that should\n","be valid assuming that we’re working with numerical data types.\n","\n","To fabricate a contrived example, let’s imagine that we found out that we mis-recorded the\n","quantity in our retail dataset and the true quantity is equal to (the current quantity * the unit\n","price) + 5. This will introduce our first numerical function as well as the pow function that raises\n","a column to the expressed power:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YoPXqUgl6fXC","outputId":"faca38c2-c7bd-49df-bc71-3de4c3230fc8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+------------------+\n","|CustomerId|      realQuantity|\n","+----------+------------------+\n","|   17850.0|239.08999999999997|\n","|   17850.0|          418.7156|\n","+----------+------------------+\n","only showing top 2 rows\n","\n"]}],"source":["from pyspark.sql.functions import expr, pow\n","fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n","df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)"]},{"cell_type":"markdown","metadata":{"id":"fL-Ow4Xi6fXC"},"source":["Notice that we were able to multiply our columns together because they were both numerical.\n","Naturally we can add and subtract as necessary, as well. In fact, we can do all of this as a SQL\n","expression, as well:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mx-QG5k6fXC","outputId":"f4221144-5c4d-4082-ede9-ad7056e03f63"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+------------------+\n","|CustomerId|      realQuantity|\n","+----------+------------------+\n","|   17850.0|239.08999999999997|\n","|   17850.0|          418.7156|\n","+----------+------------------+\n","only showing top 2 rows\n","\n"]}],"source":["# in Python\n","df.selectExpr(\n","\"CustomerId\",\n","\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)\n","#-- in SQL\n","#SELECT customerId, (POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\n","#FROM dfTable"]},{"cell_type":"markdown","metadata":{"id":"cDhlh-jK6fXC"},"source":["Another common numerical task is rounding. If you’d like to just round to a whole number,\n","oftentimes you can cast the value to an integer and that will work just fine. However, Spark also\n","has more detailed functions for performing this explicitly and to a certain level of precision. In\n","the following example, we round to one decimal place:\n","\n","By default, the round function rounds up if you’re exactly in between two numbers. You can\n","round down by using the bround:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HDwIW7X6fXC","outputId":"98722d3f-4130-4c6b-b0e8-8f92ee9ff140"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+--------------+\n","|round(2.5, 0)|bround(2.5, 0)|\n","+-------------+--------------+\n","|          3.0|           2.0|\n","|          3.0|           2.0|\n","+-------------+--------------+\n","only showing top 2 rows\n","\n"]}],"source":["from pyspark.sql.functions import lit, round, bround\n","df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)\n","#-- in SQL\n","#SELECT round(2.5), bround(2.5)"]},{"cell_type":"markdown","metadata":{"id":"AJvRTKa96fXC"},"source":["Another numerical task is to compute the correlation of two columns. For example, we can see\n","the Pearson correlation coefficient for two columns to see if cheaper things are typically bought\n","in greater quantities. We can do this through a function as well as through the DataFrame\n","statistic methods:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4lZj86I6fXC","outputId":"85fb52b4-2968-44f7-fc81-23928e99528c"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------+\n","|corr(Quantity, UnitPrice)|\n","+-------------------------+\n","|     -0.04112314436835551|\n","+-------------------------+\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import corr\n","df.stat.corr(\"Quantity\", \"UnitPrice\")\n","df.select(corr(\"Quantity\", \"UnitPrice\")).show()\n","#-- in SQL\n","#SELECT corr(Quantity, UnitPrice) FROM dfTable"]},{"cell_type":"markdown","metadata":{"id":"n9bOK_hI6fXC"},"source":["Another common task is to compute summary statistics for a column or set of columns. We can\n","use the describe method to achieve exactly this. This will take all numeric columns and\n","calculate the count, mean, standard deviation, min, and max. You should use this primarily for\n","viewing in the console because the schema might change in the future:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smccRjkd6fXC","outputId":"38c3dd2c-f604-4e4c-dd34-abd78e2393e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n","|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n","+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n","|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n","|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n","| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n","|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n","|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n","+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n","\n"]}],"source":["df.describe().show()"]},{"cell_type":"markdown","metadata":{"id":"t6wWfuD36fXC"},"source":["If you need these exact numbers, you can also perform this as an aggregation yourself by\n","importing the functions and applying them to the columns that you need:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s21LOUV06fXC"},"outputs":[],"source":["from pyspark.sql.functions import count, mean, stddev_pop, min, max"]},{"cell_type":"markdown","metadata":{"id":"6QW1P2mw6fXC"},"source":["There are a number of statistical functions available in the StatFunctions Package (accessible\n","using stat as we see in the code block below). These are DataFrame methods that you can use\n","to calculate a variety of different things. For instance, you can calculate either exact or\n","approximate quantiles of your data using the approxQuantile method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hR-mx34k6fXC","outputId":"8ab41de9-d6ca-44e0-f4bc-5bce1f0773a5"},"outputs":[{"data":{"text/plain":["[2.51]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["colName = \"UnitPrice\"\n","quantileProbs = [0.5]\n","relError = 0.05\n","df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError) # 2.51"]},{"cell_type":"markdown","metadata":{"id":"KhMbOECY6fXC"},"source":["You also can use this to see a cross-tabulation or frequent item pairs (be careful, this output will\n","be large and is omitted for this reason):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kixq9PhR6fXC","outputId":"f73949d2-b47b-4f8a-f5d8-fee717daf5e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n","|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n","+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n","|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n","|             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n","|             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n","|             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","|             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n","+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n","only showing top 20 rows\n","\n"]}],"source":["df.stat.crosstab(\"StockCode\", \"Quantity\").show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9uYM9gAr6fXD","outputId":"527d80b9-ea44-4431-8257-25224860ba44"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","| StockCode_freqItems|  Quantity_freqItems|\n","+--------------------+--------------------+\n","|[90214E, 20728, 2...|[200, 128, 23, 32...|\n","+--------------------+--------------------+\n","\n"]}],"source":["df.stat.freqItems([\"StockCode\", \"Quantity\"]).show()"]},{"cell_type":"markdown","metadata":{"id":"xJSo6T9s6fXD"},"source":["As a last note, we can also add a unique ID to each row by using the function\n","monotonically_increasing_id. This function generates a unique value for each row, starting\n","with 0:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEUs7imJ6fXD","outputId":"c7c6b000-7c87-49d4-ea12-6f0c807b0e34"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+\n","| ID|\n","+---+\n","|  0|\n","|  1|\n","|  2|\n","|  3|\n","|  4|\n","|  5|\n","|  6|\n","|  7|\n","|  8|\n","|  9|\n","+---+\n","only showing top 10 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import monotonically_increasing_id\n","df.select(monotonically_increasing_id().alias('ID')).show(10)"]},{"cell_type":"markdown","metadata":{"id":"JWrKc8Gl6fXD"},"source":["There are functions added with every release, so check the documentation for more methods. For\n","instance, there are some random data generation tools (e.g., rand(), randn()) with which you\n","can randomly generate data; however, there are potential determinism issues when doing so.\n","(You can find discussions about these challenges on the Spark mailing list.) There are also a\n","number of more advanced tasks like bloom filtering and sketching algorithms available in the\n","stat package that we mentioned (and linked to) at the beginning of this chapter. Be sure to search\n","the API documentation for more information and functions."]},{"cell_type":"markdown","metadata":{"id":"t1MagfVH6fXD"},"source":["# Working with Strings\n","String manipulation shows up in nearly every data flow, and it’s worth explaining what you can\n","do with strings. You might be manipulating log files performing regular expression extraction or\n","substitution, or checking for simple string existence, or making all strings uppercase or\n","lowercase.\n","\n","Let’s begin with the last task because it’s the most straightforward. The initcap function will\n","capitalize every word in a given string when that word is separated from another by a space."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2lSwTjO6fXD","outputId":"3c9270cf-8d17-4ec4-a5dd-ef71a7ed724e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|initcap(Description)|\n","+--------------------+\n","|White Hanging Hea...|\n","| White Metal Lantern|\n","|Cream Cupid Heart...|\n","|Knitted Union Fla...|\n","|Red Woolly Hottie...|\n","|Set 7 Babushka Ne...|\n","|Glass Star Froste...|\n","|Hand Warmer Union...|\n","|Hand Warmer Red P...|\n","|Assorted Colour B...|\n","|Poppy's Playhouse...|\n","|Poppy's Playhouse...|\n","|Feltcraft Princes...|\n","|Ivory Knitted Mug...|\n","|Box Of 6 Assorted...|\n","|Box Of Vintage Ji...|\n","|Box Of Vintage Al...|\n","|Home Building Blo...|\n","|Love Building Blo...|\n","|Recipe Box With M...|\n","+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import initcap\n","df.select(initcap(col(\"Description\"))).show()\n","#-- in SQL\n","#SELECT initcap(Description) FROM dfTable"]},{"cell_type":"markdown","metadata":{"id":"b7nW39VA6fXD"},"source":["As just mentioned, you can cast strings in uppercase and lowercase, as well:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpbuwRyk6fXD","outputId":"bdc4dc03-8b73-4013-8ead-91090c53cf04"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+-------------------------+\n","|         Description|  lower(Description)|upper(lower(Description))|\n","+--------------------+--------------------+-------------------------+\n","|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n","| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n","+--------------------+--------------------+-------------------------+\n","only showing top 2 rows\n","\n"]}],"source":["from pyspark.sql.functions import lower, upper\n","df.select(col(\"Description\"),\n","lower(col(\"Description\")),\n","upper(lower(col(\"Description\")))).show(2)\n","#-- in SQL\n","#SELECT Description, lower(Description), Upper(lower(Description)) FROM dfTable"]},{"cell_type":"markdown","metadata":{"id":"cQ1MHXeh6fXD"},"source":["Another trivial task is adding or removing spaces around a string. You can do this by using lpad,\n","ltrim, rpad and rtrim, trim:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PD0goUOn6fXD","outputId":"a9193386-3532-4672-c2d4-a32d9908edc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+------+-----+---+----------+\n","| ltrim| rtrim| trim| lp|        rp|\n","+------+------+-----+---+----------+\n","|HELLO | HELLO|HELLO|HEL|HELLO     |\n","|HELLO | HELLO|HELLO|HEL|HELLO     |\n","+------+------+-----+---+----------+\n","only showing top 2 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n","df.select(\n","ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n","rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n","trim(lit(\" HELLO \")).alias(\"trim\"),\n","lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n","rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)\n","#-- in SQL\n","#SELECT\n","#ltrim(' HELLLOOOO '),\n","#rtrim(' HELLLOOOO '),\n","#trim(' HELLLOOOO '),\n","#lpad('HELLOOOO ', 3, ' '),\n","#rpad('HELLOOOO ', 10, ' ')\n","#FROM dfTable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95ETOQxu6fXD","outputId":"e8918f8a-5404-48ff-dd8e-8270e8e8b8ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+\n","| ltrim|\n","+------+\n","|HELLO |\n","|HELLO |\n","+------+\n","only showing top 2 rows\n","\n"]}],"source":["df.select(\n","ltrim(lit(\" HELLO \")).alias(\"ltrim\")).show(2)"]},{"cell_type":"markdown","metadata":{"id":"wJ_1GhUp6fXD"},"source":["Note that if lpad or rpad takes a number less than the length of the string, it will always remove\n","values from the right side of the string.\n","## Regular Expressions\n","Probably one of the most frequently performed tasks is searching for the existence of one string\n","in another or replacing all mentions of a string with another value. This is often done with a tool\n","called regular expressions that exists in many programming languages. Regular expressions give\n","the user an ability to specify a set of rules to use to either extract values from a string or replace\n","them with some other values.\n","\n","Spark takes advantage of the complete power of Java regular expressions. The Java regular\n","expression syntax departs slightly from other programming languages, so it is worth reviewing\n","before putting anything into production. There are two key functions in Spark that you’ll need in\n","order to perform regular expression tasks: regexp_extract and regexp_replace. These\n","functions extract values and replace values, respectively.\n","\n","Let’s explore how to use the regexp_replace function to replace substitute color names in our\n","description column:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukQumVoI6fXD","outputId":"4483aa2c-7feb-4f7c-8074-2803a8792bf0"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|         color_clean|         Description|\n","+--------------------+--------------------+\n","|COLOR HANGING HEA...|WHITE HANGING HEA...|\n","| COLOR METAL LANTERN| WHITE METAL LANTERN|\n","+--------------------+--------------------+\n","only showing top 2 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import regexp_replace\n","regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n","df.select(\n","regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n","col(\"Description\")).show(2)\n","#-- in SQL\n","#SELECT\n","#regexp_replace(Description, 'BLACK|WHITE|RED|GREEN|BLUE', 'COLOR') as\n","#color_clean, Description\n","#FROM dfTable"]},{"cell_type":"markdown","metadata":{"id":"8hj2TKBm6fXD"},"source":["Another task might be to replace given characters with other characters. Building this as a\n","regular expression could be tedious, so Spark also provides the translate function to replace these\n","values. This is done at the character level and will replace all instances of a character with the\n","indexed character in the replacement string:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFzjd6RS6fXD","outputId":"9c141d04-2ba6-4537-96ab-264c9b3e1507"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------+--------------------+\n","|translate(Description, LEET, 1357)|         Description|\n","+----------------------------------+--------------------+\n","|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n","|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n","+----------------------------------+--------------------+\n","only showing top 2 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import translate\n","df.select(translate(col(\"Description\"), \"LEET\", \"1357\"),col(\"Description\"))\\\n",".show(2)\n","#-- in SQL\n","#SELECT translate(Description, 'LEET', '1337'), Description FROM dfTable"]},{"cell_type":"markdown","metadata":{"id":"mqz9IPU76fXE"},"source":["We can also perform something similar, like pulling out the first mentioned color:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urY-dTLw6fXE","outputId":"cf2333a7-78e1-4df0-9497-4ef0bfe91123"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+--------------------+\n","|color_clean|         Description|\n","+-----------+--------------------+\n","|      WHITE|WHITE HANGING HEA...|\n","|      WHITE| WHITE METAL LANTERN|\n","+-----------+--------------------+\n","only showing top 2 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import regexp_extract\n","extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n","df.select(\n","regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n","col(\"Description\")).show(2)\n","#-- in SQL\n","#SELECT regexp_extract(Description, '(BLACK|WHITE|RED|GREEN|BLUE)', 1),\n","#Description\n","#FROM dfTable"]},{"cell_type":"markdown","metadata":{"id":"OiSTekDC6fXE"},"source":["Sometimes, rather than extracting values, we simply want to check for their existence. We can do\n","this with the contains method on each column. This will return a Boolean declaring whether the\n","value you specify is in the column’s string (In Python and SQL, we can use the instr function):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYj0yudl6fXE","outputId":"2bfe1a44-dd61-4dc6-cd42-609abbdec9c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------+\n","|Description                       |\n","+----------------------------------+\n","|WHITE HANGING HEART T-LIGHT HOLDER|\n","|WHITE METAL LANTERN               |\n","|RED WOOLLY HOTTIE WHITE HEART.    |\n","+----------------------------------+\n","only showing top 3 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import instr\n","containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n","containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n","df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",".where(\"hasSimpleColor\")\\\n",".select(\"Description\").show(3, False)\n","#-- in SQL\n","#SELECT Description FROM dfTable\n","#WHERE instr(Description, 'BLACK') >= 1 OR instr(Description, 'WHITE') >= 1"]},{"cell_type":"markdown","metadata":{"id":"IgH-YZoY6fXE"},"source":["This is trivial with just two values, but it becomes more complicated when there are values.\n","\n","Let’s work through this in a more rigorous way and take advantage of Spark’s ability to accept a\n","dynamic number of arguments. When we convert a list of values into a set of arguments and pass\n","them into a function, we use a language feature called varargs. Using this feature, we can\n","effectively unravel an array of arbitrary length and pass it as arguments to a function. This,\n","coupled with select makes it possible for us to create arbitrary numbers of columns\n","dynamically:\n","\n","In Python, we’re going to use a different function,\n","locate, that returns the integer location (1 based location). We then convert that to a Boolean\n","before using it as the same basic feature:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQ4u5Fem6fXE","outputId":"3b05e6bc-481f-4bcd-d7b4-2369ef598bda"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------+\n","|Description                       |\n","+----------------------------------+\n","|WHITE HANGING HEART T-LIGHT HOLDER|\n","|WHITE METAL LANTERN               |\n","|RED WOOLLY HOTTIE WHITE HEART.    |\n","+----------------------------------+\n","only showing top 3 rows\n","\n"]}],"source":["from pyspark.sql.functions import expr, locate\n","simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n","def color_locator(column, color_string):\n","    return locate(color_string.upper(), column)\\\n","    .cast(\"boolean\")\\\n","    .alias(\"is_\" + color_string)\n","selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n","selectedColumns.append(expr(\"*\")) # has to a be Column type\n","df.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n",".select(\"Description\").show(3, False)"]},{"cell_type":"markdown","metadata":{"id":"WQMNOZpv6fXE"},"source":["This simple feature can often help you programmatically generate columns or Boolean filters in a\n","way that is simple to understand and extend. We could extend this to calculating the smallest\n","common denominator for a given input value, or whether a number is a prime.\n","# Working with Dates and Timestamps\n","Spark, as we saw with our current dataset, will make a best effort to\n","correctly identify column types, including dates and timestamps when we enable inferSchema\n","\n","As we hinted earlier, working with dates and timestamps closely relates to working with strings\n","because we often store our timestamps or dates as strings and convert them into date types at\n","runtime. This is less common when working with databases and structured data but much more\n","common when we are working with text and CSV files.\n","\n","Although Spark will do read dates or times on a best-effort basis. However, sometimes there will\n","be no getting around working with strangely formatted dates and times. The key to\n","understanding the transformations that you are going to need to apply is to ensure that you know\n","exactly what type and format you have at each given step of the way. Another common “gotcha”\n","is that Spark’s TimestampType class supports only second-level precision, which means that if\n","you’re going to be working with milliseconds or microseconds, you’ll need to work around this\n","problem by potentially operating on them as longs. Any more precision when coercing to a\n","TimestampType will be removed.\n","\n","Spark can be a bit particular about what format you have at any given point in time. It’s\n","important to be explicit when parsing or converting to ensure that there are no issues in doing so.\n","At the end of the day, Spark is working with Java dates and timestamps and therefore conforms\n","to those standards. Let’s begin with the basics and get the current date and the current\n","timestamps:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WP9a0e26fXE","outputId":"088fad84-95ae-40a1-cef7-cbba8da68794"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- id: long (nullable = false)\n"," |-- today: date (nullable = false)\n"," |-- now: timestamp (nullable = false)\n","\n"]}],"source":["from pyspark.sql.functions import current_date, current_timestamp\n","dateDF = spark.range(10)\\\n",".withColumn(\"today\", current_date())\\\n",".withColumn(\"now\", current_timestamp())\n","dateDF.createOrReplaceTempView(\"dateTable\")\n","dateDF.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"l7Rwj7MI6fXE"},"source":["Now that we have a simple DataFrame to work with, let’s add and subtract five days from today.\n","These functions take a column and then the number of days to either add or subtract as the\n","arguments:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsljQpJs6fXE","outputId":"54aef318-f24b-46f4-93e0-e3bf26cb1d56"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------+------------------+\n","|date_sub(today, 5)|date_add(today, 5)|\n","+------------------+------------------+\n","|        2020-09-26|        2020-10-06|\n","+------------------+------------------+\n","only showing top 1 row\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import date_add, date_sub\n","dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n","#-- in SQL\n","#SELECT date_sub(today, 5), date_add(today, 5) FROM dateTable"]},{"cell_type":"markdown","metadata":{"id":"GdkgaSb46fXE"},"source":["Another common task is to take a look at the difference between two dates. We can do this with\n","the datediff function that will return the number of days in between two dates. Most often we\n","just care about the days, and because the number of days varies from month to month, there also\n","exists a function, months_between, that gives you the number of months between two dates:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DUuRToG6fXE","outputId":"dc0aa919-c965-4fc9-aac1-eb7987bc6f37"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------+\n","|datediff(week_ago, today)|\n","+-------------------------+\n","|                       -7|\n","+-------------------------+\n","only showing top 1 row\n","\n","+--------------------------------+\n","|months_between(start, end, true)|\n","+--------------------------------+\n","|                    -16.67741935|\n","+--------------------------------+\n","only showing top 1 row\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import datediff, months_between, to_date\n","dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",".select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n","dateDF.select(\n","to_date(lit(\"2016-01-01\")).alias(\"start\"),\n","to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",".select(months_between(col(\"start\"), col(\"end\"))).show(1)\n","#-- in SQL\n","#SELECT to_date('2016-01-01'), months_between('2016-01-01', '2017-01-01'),\n","#datediff('2016-01-01', '2017-01-01')\n","#FROM dateTable"]},{"cell_type":"markdown","metadata":{"id":"w3nmkNmw6fXE"},"source":["Notice that we introduced a new function: the to_date function. The to_date function allows\n","you to convert a string to a date, optionally with a specified format. We specify our format in the\n","Java SimpleDateFormat which will be important to reference if you use this function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OARma6g6fXE","outputId":"0ba1f3e1-bc0f-4bfd-a3f0-ea152d309d6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+\n","|to_date(`date`)|\n","+---------------+\n","|     2017-01-01|\n","+---------------+\n","only showing top 1 row\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import to_date, lit\n","spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",".select(to_date(col(\"date\"))).show(1)"]},{"cell_type":"markdown","metadata":{"id":"vr8_SZoD6fXE"},"source":["Spark will not throw an error if it cannot parse the date; rather, it will just return null. This can\n","be a bit tricky in larger pipelines because you might be expecting your data in one format and\n","getting it in another. To illustrate, let’s take a look at the date format that has switched from yearmonth-\n","day to year-day-month. Spark will fail to parse this date and silently return null instead:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CS3dklc6fXE","outputId":"b5ed2f25-6736-4470-e270-2b131a59e6de"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------+---------------------+\n","|to_date('2016-20-12')|to_date('2017-12-11')|\n","+---------------------+---------------------+\n","|                 null|           2017-12-11|\n","+---------------------+---------------------+\n","only showing top 1 row\n","\n"]}],"source":["dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"]},{"cell_type":"markdown","metadata":{"id":"uCH4r2km6fXF"},"source":["We find this to be an especially tricky situation for bugs because some dates might match the\n","correct format, whereas others do not. In the previous example, notice how the second date\n","appears as Decembers 11th instead of the correct day, November 12th. Spark doesn’t throw an\n","error because it cannot know whether the days are mixed up or that specific row is incorrect.\n","\n","Let’s fix this pipeline, step by step, and come up with a robust way to avoid these issues entirely.\n","The first step is to remember that we need to specify our date format according to the Java\n","SimpleDateFormat standard.\n","\n","We will use two functions to fix this: to_date and to_timestamp. The former optionally\n","expects a format, whereas the latter requires one:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfWp1IfX6fXF"},"outputs":[],"source":["# in Python\n","from pyspark.sql.functions import to_date\n","dateFormat = \"yyyy-dd-MM\"\n","cleanDateDF = spark.range(1).select(\n","to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n","to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n","cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n","#-- in SQL\n","#SELECT to_date(date, 'yyyy-dd-MM'), to_date(date2, 'yyyy-dd-MM'), to_date(date)\n","#FROM dateTable2"]},{"cell_type":"markdown","metadata":{"id":"senVXb6p6fXF"},"source":["Now let’s use an example of to_timestamp, which always requires a format to be specified:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaVx5ENB6fXF","outputId":"2f9a75ce-91b6-48eb-8909-f274dad16b2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------+\n","|to_timestamp(`date`, 'yyyy-dd-MM')|\n","+----------------------------------+\n","|               2017-11-12 00:00:00|\n","+----------------------------------+\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import to_timestamp\n","cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()\n","#-- in SQL\n","#SELECT to_timestamp(date, 'yyyy-dd-MM'), to_timestamp(date2, 'yyyy-dd-MM')\n","#FROM dateTable2"]},{"cell_type":"markdown","metadata":{"id":"LcPx2ceF6fXF"},"source":["After we have our date or timestamp in the correct format and type, comparing between them is\n","actually quite easy. We just need to be sure to either use a date/timestamp type or specify our\n","string according to the right format of yyyy-MM-dd if we’re comparing a date:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YNVHQvLg6fXF","outputId":"53b87541-78a0-4ae7-e48f-759de7539787"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+----------+\n","|      date|     date2|\n","+----------+----------+\n","|2017-11-12|2017-12-20|\n","+----------+----------+\n","\n"]}],"source":["cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"]},{"cell_type":"markdown","metadata":{"id":"25V8sGqK6fXF"},"source":["One minor point is that we can also set this as a string, which Spark parses to a literal:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NjiJooom6fXF","outputId":"dbeaa3a0-5b0c-4d78-f570-a3a8961c27e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+----------+\n","|      date|     date2|\n","+----------+----------+\n","|2017-11-12|2017-12-20|\n","+----------+----------+\n","\n"]}],"source":["cleanDateDF.filter(col(\"date2\") > \"2017-12-12\").show()"]},{"cell_type":"markdown","metadata":{"id":"WP99gAAa6fXF"},"source":["# Working with Nulls in Data\n","As a best practice, you should always use nulls to represent missing or empty data in your\n","DataFrames. Spark can optimize working with null values more than it can if you use empty\n","strings or other values. The primary way of interacting with null values, at DataFrame scale, is to\n","use the .na subpackage on a DataFrame. There are also several functions for performing\n","operations and explicitly specifying how Spark should handle null values. For more information,\n","see Chapter 5 (where we discuss ordering), and also refer back to “Working with Booleans”.\n","\n","## WARNING\n","Nulls are a challenging part of all programming, and Spark is no exception. In our opinion, being\n","explicit is always better than being implicit when handling null values. For instance, in this part of the\n","book, we saw how we can define columns as having null types. However, this comes with a catch.\n","When we declare a column as not having a null time, that is not actually enforced. To reiterate, when\n","you define a schema in which all columns are declared to not have null values, Spark will not enforce\n","that and will happily let null values into that column. The nullable signal is simply to help Spark SQL\n","optimize for handling that column. If you have null values in columns that should not have null values,\n","you can get an incorrect result or see strange exceptions that can be difficult to debug.\n","\n","There are two things you can do with null values: you can explicitly drop nulls or you can fill\n","them with a value (globally or on a per-column basis). Let’s experiment with each of these now.\n","\n","## Coalesce\n","Spark includes a function to allow you to select the first non-null value from a set of columns by\n","using the coalesce function. In this case, there are no null values, so it simply returns the first\n","column:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57Ezlo_W6fXF","outputId":"22b2089e-528b-42db-9456-ba7b5090d361"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------------------+\n","|coalesce(Description, CustomerId)|\n","+---------------------------------+\n","|             WHITE HANGING HEA...|\n","|              WHITE METAL LANTERN|\n","|             CREAM CUPID HEART...|\n","|             KNITTED UNION FLA...|\n","|             RED WOOLLY HOTTIE...|\n","|             SET 7 BABUSHKA NE...|\n","|             GLASS STAR FROSTE...|\n","|             HAND WARMER UNION...|\n","|             HAND WARMER RED P...|\n","|             ASSORTED COLOUR B...|\n","|             POPPY'S PLAYHOUSE...|\n","|             POPPY'S PLAYHOUSE...|\n","|             FELTCRAFT PRINCES...|\n","|             IVORY KNITTED MUG...|\n","|             BOX OF 6 ASSORTED...|\n","|             BOX OF VINTAGE JI...|\n","|             BOX OF VINTAGE AL...|\n","|             HOME BUILDING BLO...|\n","|             LOVE BUILDING BLO...|\n","|             RECIPE BOX WITH M...|\n","+---------------------------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import coalesce\n","df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"]},{"cell_type":"markdown","metadata":{"id":"UO0wK6Fu6fXF"},"source":["## drop\n","The simplest function is drop, which removes rows that contain nulls. The default is to drop any\n","row in which any value is null:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxJj7Fbs6fXF","outputId":"3ff826d5-1c60-4a09-81b5-26422e8778b7"},"outputs":[{"data":{"text/plain":["DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["df.na.drop()\n","df.na.drop(\"any\")"]},{"cell_type":"markdown","metadata":{"id":"XiyVjMql6fXF"},"source":["Specifying \"any\" as an argument drops a row if any of the values are null. Using “all” drops the\n","row only if all values are null or NaN for that row:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxqtPQLi6fXF","outputId":"fb611abf-ab17-4ebc-9127-a5d80dd7d8fd"},"outputs":[{"data":{"text/plain":["DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["df.na.drop(\"all\")"]},{"cell_type":"markdown","metadata":{"id":"_o2CbsSG6fXF"},"source":["We can also apply this to certain sets of columns by passing in an array of columns:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2p8Lbvtl6fXF","outputId":"b669cc47-f347-4bf6-f646-0d8593ce8d8a"},"outputs":[{"data":{"text/plain":["DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"]},{"cell_type":"markdown","metadata":{"id":"Ow-dTsYk6fXF"},"source":["## fill\n","Using the fill function, you can fill one or more columns with a set of values. This can be done\n","by specifying a map—that is a particular value and a set of columns.\n","For example, to fill all null values in columns of type String, you might specify the following:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2Ck829s6fXG","outputId":"2163992a-2c57-472d-f208-5f1622a3dccb"},"outputs":[{"data":{"text/plain":["DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["df.na.fill(\"All Null values become this string\")"]},{"cell_type":"markdown","metadata":{"id":"mn4eV4-T6fXG"},"source":["We could do the same for columns of type Integer by using df.na.fill(5:Integer), or for\n","Doubles df.na.fill(5:Double). To specify columns, we just pass in an array of column names\n","like we did in the previous example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9UE3sSP6fXG","outputId":"06ff0078-75d0-4afc-9e12-cdb9caa48d47"},"outputs":[{"data":{"text/plain":["DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"]},{"cell_type":"markdown","metadata":{"id":"00UfRio_6fXG"},"source":["We can also do this with with a Scala Map, where the key is the column name and the value is the\n","value we would like to use to fill null values:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gi4N76566fXG","outputId":"9b5bea56-b5fb-4e3d-dffe-1257049873f6"},"outputs":[{"data":{"text/plain":["DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["fill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\n","df.na.fill(fill_cols_vals)"]},{"cell_type":"markdown","metadata":{"id":"R6c3svFL6fXG"},"source":["## replace\n","In addition to replacing null values like we did with drop and fill, there are more flexible\n","options that you can use with more than just null values. Probably the most common use case is\n","to replace all values in a certain column according to their current value. The only requirement is\n","that this value be the same type as the original value:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_ATwsV16fXG","outputId":"b7d7a143-90fe-4e4c-e5e6-ea90135a0312"},"outputs":[{"data":{"text/plain":["DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"]},{"cell_type":"markdown","metadata":{"id":"norBWSd_6fXG"},"source":["# Working with Complex Types\n","Complex types can help you organize and structure your data in ways that make more sense for\n","the problem that you are hoping to solve. There are three kinds of complex types: structs, arrays, and maps.\n","## Structs\n","You can think of structs as DataFrames within DataFrames. A worked example will illustrate\n","this more clearly. We can create a struct by wrapping a set of columns in parenthesis in a query:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upLzXgmn6fXG","outputId":"a0738539-ced6-45fe-f073-ca9cec9f7e80"},"outputs":[{"data":{"text/plain":["DataFrame[complex: struct<Description:string,InvoiceNo:string>, InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")\n","df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWt6k2ux6fXG"},"outputs":[],"source":["from pyspark.sql.functions import struct\n","complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n","complexDF.createOrReplaceTempView(\"complexDF\")"]},{"cell_type":"markdown","metadata":{"id":"LRF0X9wJ6fXG"},"source":["We now have a DataFrame with a column complex. We can query it just as we might another\n","DataFrame, the only difference is that we use a dot syntax to do so, or the column method\n","getField:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmYmbMdW6fXG","outputId":"3f09394a-e58b-44c4-d6b7-4c27e311c3d4"},"outputs":[{"data":{"text/plain":["DataFrame[complex.Description: string]"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["complexDF.select(\"complex.Description\")\n","complexDF.select(col(\"complex\").getField(\"Description\"))"]},{"cell_type":"markdown","metadata":{"id":"a5k7Ew5m6fXG"},"source":["We can also query all values in the struct by using *. This brings up all the columns to the toplevel\n","DataFrame:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WrM65qK6fXG","outputId":"d7936cff-8128-4367-82cd-4b84d0c441aa"},"outputs":[{"data":{"text/plain":["DataFrame[Description: string, InvoiceNo: string]"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["complexDF.select(\"complex.*\")\n","#-- in SQL\n","#SELECT complex.* FROM complexDF"]},{"cell_type":"markdown","metadata":{"id":"8pICUhRM6fXG"},"source":["## Arrays\n","To define arrays, let’s work through a use case. With our current data, our objective is to take\n","every single word in our Description column and convert that into a row in our DataFrame.\n","The first task is to turn our Description column into a complex type, an array.\n","### split\n","We do this by using the split function and specify the delimiter:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfphCJLj6fXG","outputId":"2fbbab05-c239-43c8-8a52-c7bb917bbca5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------+\n","|split(Description,  , -1)|\n","+-------------------------+\n","|     [WHITE, HANGING, ...|\n","|     [WHITE, METAL, LA...|\n","+-------------------------+\n","only showing top 2 rows\n","\n"]}],"source":["from pyspark.sql.functions import split\n","df.select(split(col(\"Description\"), \" \")).show(2)\n","#-- in SQL\n","#SELECT split(Description, ' ') FROM dfTable"]},{"cell_type":"markdown","metadata":{"id":"-p4TZSTJ6fXG"},"source":["This is quite powerful because Spark allows us to manipulate this complex type as another\n","column. We can also query the values of the array using Python-like syntax:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AL-YHnKz6fXG","outputId":"791e09b4-d58d-4106-e3ea-3d75b14d22f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+\n","|array_col[0]|\n","+------------+\n","|       WHITE|\n","|       WHITE|\n","+------------+\n","only showing top 2 rows\n","\n"]}],"source":["# in Python\n","df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",".selectExpr(\"array_col[0]\").show(2)\n","#-- in SQL\n","#SELECT split(Description, ' ')[0] FROM dfTable"]},{"cell_type":"markdown","metadata":{"id":"78a2m9Kl6fXG"},"source":["### Array Length\n","We can determine the array’s length by querying for its size:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27QczEfA6fXG","outputId":"ad7c75b3-3335-455a-9a87-60f62c31c9a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------------+\n","|size(split(Description,  , -1))|\n","+-------------------------------+\n","|                              5|\n","|                              3|\n","+-------------------------------+\n","only showing top 2 rows\n","\n"]}],"source":["from pyspark.sql.functions import size\n","df.select(size(split(col(\"Description\"), \" \"))).show(2) # shows 5 and 3"]},{"cell_type":"markdown","metadata":{"id":"amgolUrP6fXG"},"source":["### array_contains\n","We can also see whether this array contains a value:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9AfDdiw-6fXH","outputId":"6f4d5930-f305-4d45-977a-e3b0fc96ad48"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------+\n","|array_contains(split(Description,  , -1), WHITE)|\n","+------------------------------------------------+\n","|                                            true|\n","|                                            true|\n","+------------------------------------------------+\n","only showing top 2 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import array_contains\n","df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)\n","#-- in SQL\n","#SELECT array_contains(split(Description, ' '), 'WHITE') FROM dfTable"]},{"cell_type":"markdown","metadata":{"id":"wswSYPc06fXH"},"source":["However, this does not solve our current problem. To convert a complex type into a set of rows\n","(one per value in our array), we need to use the explode function.\n","### explode\n","The explode function takes a column that consists of arrays and creates one row (with the rest of\n","the values duplicated) per value in the array. Figure 6-1 illustrates the process.\n","![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-V-bV8b6fXH","outputId":"b6cd3de7-5746-4e68-bc98-56da5e6de704"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+---------+--------+\n","|         Description|InvoiceNo|exploded|\n","+--------------------+---------+--------+\n","|WHITE HANGING HEA...|   536365|   WHITE|\n","|WHITE HANGING HEA...|   536365| HANGING|\n","+--------------------+---------+--------+\n","only showing top 2 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import split, explode\n","df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",".select(\"Description\", \"InvoiceNo\", \"exploded\").show(2)\n","#-- in SQL\n","#SELECT Description, InvoiceNo, exploded\n","#FROM (SELECT *, split(Description, \" \") as splitted FROM dfTable)\n","#LATERAL VIEW explode(splitted) as exploded"]},{"cell_type":"markdown","metadata":{"id":"jofcSEAu6fXH"},"source":["### Maps\n","Maps are created by using the map function and key-value pairs of columns. You then can select\n","them just like you might select from an array:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-ITOlNV6fXH","outputId":"0f5052f1-6b27-4a2d-caee-e749205398ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|         complex_map|\n","+--------------------+\n","|[WHITE HANGING HE...|\n","|[WHITE METAL LANT...|\n","+--------------------+\n","only showing top 2 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import create_map\n","df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",".show(2)\n","#-- in SQL\n","#SELECT map(Description, InvoiceNo) as complex_map FROM dfTable\n","#WHERE Description IS NOT NULL"]},{"cell_type":"markdown","metadata":{"id":"fwNq02Eo6fXH"},"source":["You can query them by using the proper key. A missing key returns null:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-l5kQkq36fXH","outputId":"c056373d-6361-4a94-cb47-703a61c63835"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------+\n","|complex_map[WHITE METAL LANTERN]|\n","+--------------------------------+\n","|                            null|\n","|                          536365|\n","+--------------------------------+\n","only showing top 2 rows\n","\n"]}],"source":["# in Python\n","from pyspark.sql.functions import lit, col, create_map\n","df.select(create_map(df[\"Description\"], df[\"InvoiceNo\"]).alias(\"complex_map\"))\\\n",".selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSbQvG4J6fXH","outputId":"459dfa82-d4dd-4be2-e0c0-064511774e70"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+------+\n","|                 key| value|\n","+--------------------+------+\n","|WHITE HANGING HEA...|536365|\n","| WHITE METAL LANTERN|536365|\n","+--------------------+------+\n","only showing top 2 rows\n","\n"]}],"source":["df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",".selectExpr(\"explode(complex_map)\").show(2)"]},{"cell_type":"markdown","metadata":{"id":"dowKLeQI6fXH"},"source":["### Working with JSON\n","Spark has some unique support for working with JSON data. You can operate directly on strings\n","of JSON in Spark and parse from JSON or extract JSON objects. Let’s begin by creating a JSON\n","column:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lymhnumv6fXH"},"outputs":[],"source":["jsonDF = spark.range(1).selectExpr(\"\"\"\n","'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"_R5SRiQN6fXH"},"source":["You can use the get_json_object to inline query a JSON object, be it a dictionary or array.\n","You can use json_tuple if this object has only one level of nesting:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4PSb67x6fXH","outputId":"45e2c08d-a860-4344-ee7e-a8ab224175b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+--------------------+\n","|column|                  c0|\n","+------+--------------------+\n","|     2|{\"myJSONValue\":[1...|\n","+------+--------------------+\n","\n"]}],"source":["from pyspark.sql.functions import get_json_object, json_tuple\n","jsonDF.select(\n","get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n","json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2)"]},{"cell_type":"markdown","metadata":{"id":"IYurCpz86fXH"},"source":["You can also turn a StructType into a JSON string by using the to_json function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1OkXjys76fXH","outputId":"3229f1f4-c4df-41d7-8bc0-7887dfaa8a4d"},"outputs":[{"data":{"text/plain":["DataFrame[to_json(myStruct): string]"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["from pyspark.sql.functions import to_json\n","df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",".select(to_json(col(\"myStruct\")))"]},{"cell_type":"markdown","metadata":{"id":"FVQaZIuY6fXH"},"source":["This function also accepts a dictionary (map) of parameters that are the same as the JSON data\n","source. You can use the from_json function to parse this (or other JSON data) back in. This\n","naturally requires you to specify a schema, and optionally you can specify a map of options, as\n","well:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNTgZ8_r6fXH","outputId":"cd7eae18-544d-47ac-f966-cecac10f9bc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|  from_json(newJSON)|             newJSON|\n","+--------------------+--------------------+\n","|[536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n","|[536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n","+--------------------+--------------------+\n","only showing top 2 rows\n","\n"]}],"source":["from pyspark.sql.functions import from_json\n","from pyspark.sql.types import *\n","parseSchema = StructType((\n","StructField(\"InvoiceNo\",StringType(),True),\n","StructField(\"Description\",StringType(),True)))\n","df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",".select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",".select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)"]},{"cell_type":"markdown","metadata":{"id":"P0XErc-E6fXH"},"source":["# User-Defined Functions\n","One of the most powerful things that you can do in Spark is define your own functions. These\n","user-defined functions (UDFs) make it possible for you to write your own custom\n","transformations using Python or Scala and even use external libraries. UDFs can take and return\n","one or more columns as input. Spark UDFs are incredibly powerful because you can write them\n","in several different programming languages; you do not need to create them in an esoteric format\n","or domain-specific language. They’re just functions that operate on the data, record by record.\n","By default, these functions are registered as temporary functions to be used in that specific\n","SparkSession or Context.\n","\n","Although you can write UDFs in Scala, Python, or Java, there are performance considerations\n","that you should be aware of. To illustrate this, we’re going to walk through exactly what happens\n","when you create UDF, pass that into Spark, and then execute code using that UDF.\n","\n","The first step is the actual function. We’ll create a simple one for this example. Let’s write a\n","power3 function that takes a number and raises it to a power of three:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AACDCP1V6fXH","outputId":"21ea7773-8088-414e-f70c-b413cc728ed6"},"outputs":[{"data":{"text/plain":["8.0"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["udfExampleDF = spark.range(5).toDF(\"num\")\n","def power3(double_value):\n","    return double_value ** 3\n","power3(2.0)"]},{"cell_type":"markdown","metadata":{"id":"dp2CPg0J6fXH"},"source":["In this trivial example, we can see that our functions work as expected. We are able to provide an\n","individual input and produce the expected result (with this simple test case). Thus far, our\n","expectations for the input are high: it must be a specific type and cannot be a null value (see\n","“Working with Nulls in Data”).\n","\n","Now that we’ve created these functions and tested them, we need to register them with Spark so\n","that we can use them on all of our worker machines. Spark will serialize the function on the\n","driver and transfer it over the network to all executor processes. This happens regardless of\n","language.\n","\n","When you use the function, there are essentially two different things that occur. If the function is\n","written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that\n","there will be little performance penalty aside from the fact that you can’t take advantage of code\n","generation capabilities that Spark has for built-in functions. There can be performance issues if\n","you create or use a lot of objects; we cover that in the section on optimization in Chapter 19.\n","\n","If the function is written in Python, something quite different happens. Spark starts a Python\n","process on the worker, serializes all of the data to a format that Python can understand\n","(remember, it was in the JVM earlier), executes the function row by row on that data in the\n","Python process, and then finally returns the results of the row operations to the JVM and Spark.\n","Figure 6-2 provides an overview of the process.\n","![image.png](attachment:image.png)\n","\n","## WARNING\n","Starting this Python process is expensive, but the real cost is in serializing the data to Python. This is\n","costly for two reasons: it is an expensive computation, but also, after the data enters Python, Spark\n","cannot manage the memory of the worker. This means that you could potentially cause a worker to fail\n","if it becomes resource constrained (because both the JVM and Python are competing for memory on\n","the same machine). We recommend that you write your UDFs in Scala or Java—the small amount of\n","time it should take you to write the function in Scala will always yield significant speed ups, and on\n","top of that, you can still use the function from Python!\n","\n","Now that you have an understanding of the process, let’s work through an example. First, we\n","need to register the function to make it available as a DataFrame function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5csLtIiy6fXI","outputId":"4947bcbc-3975-4fdb-a2e0-83d711f8087d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+\n","|power3(num)|\n","+-----------+\n","|          0|\n","|          1|\n","+-----------+\n","only showing top 2 rows\n","\n"]}],"source":["# Python—first, we register it:\n","# in Python\n","from pyspark.sql.functions import udf\n","power3udf = udf(power3)\n","#Then, we can use it in our DataFrame code:\n","# in Python\n","from pyspark.sql.functions import col\n","udfExampleDF.select(power3udf(col(\"num\"))).show(2)"]},{"cell_type":"markdown","metadata":{"id":"CFJHfElI6fXI"},"source":["# Conclusion\n","This chapter demonstrated how easy it is to extend Spark SQL to your own purposes and do so in\n","a way that is not some esoteric, domain-specific language but rather simple functions that are\n","easy to test and maintain without even using Spark! This is an amazingly powerful tool that you\n","can use to specify sophisticated business logic that can run on five rows on your local machines\n","or on terabytes of data on a 100-node cluster!"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}